---
subtitle: "TMA4268 Statistical Learning V2018"
title: "Compulsory exercise 2: Problem 3"
author: "NN1, NN2 and NN3"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: #3rd letter intentation hierarchy - uncomment html_document and comment pdf_document if neede
  # html_document
  pdf_document
---
  
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,results="hold",message = FALSE,warning=FALSE)
```

You need to install the following packages in R:

```{r,eval=FALSE,echo=TRUE}
install.packages("ISLR")
install.packages("ggplot2")
install.packages("GGally")
install.packages("leaps")
install.packages("glmnet")
install.packages("gam")
```

# Problem 1 - Model selection and cross-validation 


## c) Cross-validation [1 point]

* Q6: $k$-fold cross validation is performed by randomly dividing the data set consisting of $n$ observations into $k$ approximately equally sized groups, or $folds$. The first fold is left out and kept as a validation set, and the remaining $k-1$ folds are used to fit the model. Then the validation set is used to calculate the $MSE$. The process is repated $k$ times, so the $MSE_i$ is calculated for fold $i=1,...,k$. If $n$ is a multiple of $k$, we have $n_k=n/k$ and the formula for $MSE_k$ is

$$
MSE_k=\frac{1}{n_k}\sum_{i \in C_k}(y_i-\hat y_i)^2,
$$
where $C_k$ denotes the held-out fold $k$, $\hat y_i$ is the fit obtained when holding out data from $C_k$, and $n_k$ is the number of observations in fold $k$. 

The final CV estimate is the average of all the $k$ MSEs:

$$
CV_k=\frac{1}{n}\sum_{i=j}^k n_k MSE_k

$$

* Q7. Why may $k$-fold cross-validation be preferred to leave-one-out cross-validation?
LOOCV is k-fold CV for $k=n$, so LOOCV is usually computationally more expensive than $k$-fold when $k$ is substantially smaller than $n$. This is because for the LOOCV $n$ different models is fitted, as opposed to $k=5$ or $k=10$ that is most common $k$-fold CV in practice. 

Another reason for preferring $k$-fold CV over LOOCV comes from the bias-variance trade-off. LOOCV will have a low bias, but a rather high variance comapred to $k$-fold CV. This follows from the procedure of cross validation. One averages over model fits that consists of almost the same set of observations; only one observation differ for each model. This will cause high correlation between each model. When performing LOOCV, one is in general averaging over the models, and the higher correlated these are, the higher will the variance be. From this rationale, the models in $k$-fold cross-validation will be trained on less overlapping training data and will have less variance than the LOOCV.


## d) Programming $10$-fold cross-validation in R [2 points]

Refer to pages 248-249 in our textbook "Introduction to Statistical Learning", and also [solutions to Recommended Problem 5 in Module 6](LINK) for hints on how to make the R code.

* Q8. Write R code to perform $10$-fold cross-validation with the best subset method on `ourAutoTrain` (no, do not use `ourAutoTest` here). (Hint: smart to make a `predict.regsubsets` function as on page 249 in "Introduction to Statistical Learning".)
* Q9. What is the optimal model complexity (number of parameters) in your regression? 
* Q10. Use the model complexity you found in Q9 to find the best model on the `ourAutoTrain`. Report interesting features of this final model.  Also use this model fit to predict new values for `ourAutoTest` and report the MSE. (If you get the same best model as Q4, just refer back to Q4 and Q5.)

