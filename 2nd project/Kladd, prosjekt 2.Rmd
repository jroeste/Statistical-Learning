---
output: html_document
editor_options: 
  chunk_output_type: console
---
##2a) Explain figures

* Q11: The first figure corresponds to Lasso, and the second figure corresponds to Ridge regression. 

This can be concluded from the fact that the coefficient values in the first figure becomes zero for large enough value of $\lambda$, which is only possible for Lasso and not for Ridge regression. In Ridge regression the coefficient values only approaches zero as $\lambda$ grows, which is corresponding to figure 2.  

* Q12: The tuning parameter penalizes values of the estimated coefficients $\beta$, resulting in a reduction of the sum of the coefficient absolute values. This reduction increases with the value of $\lambda$.

This can be seen from the formulas, as they include the terms $\lambda \sum_{j=1}^p \beta_j^2$ and $\lambda \sum_{j=1}^p |\beta_j|$ respectively in ridge regression and lasso. These terms favours $\beta$-values close to zero, as the $\beta$-values should be chosen to minimize the expressions for the two models. 

This can also be seen from the figures as these displays how the coefficient values vary with $\lambda$. The standardized coefficients tends towards zero as $\lambda$ grows. 

Increasing lambda will result in less flexibility of the model. As a consequence, increasing lamba means reducing variance and increasing bias. 

When $\lambda \rightarrow \infty$ the $\beta$-values will approach zero in Ridge regression and become zero in Lasso in order to minimize the model expressions. When $\lambda = 0$ both Ridge regression and Lasso will produce the same output as ordinary least squares estimate, as the penalty term is removed.

* Q13:

For Lasso the coefficients eventually become zero when $\lambda$ is large enough, and thus the method results in model selection. In Ridge regression the coefficients will not become zero, and thus it does not perform model selection. 

In problem 1b BIC is used to compare models with different number of included variables. Weight, year and cylinders are the variables that are included in most of the models in 1b. The best model according to the method in 1b is that where only the acceleration variable is excluded. In figure 1 both year and cylinders need a higher $\lambda$-value to become zero than most of the other variables. Weight has a very low coefficient value, and thus it it difficult to see when it becomes zero. However, it seems like the same trends of reletive importancy of the variables are captured in both methods. On the other hand, for a $\lambda$-value above 2, Lasso has set almost all of the variables to zero. Thus for a high $\lambda$-value, the method gives quite different results. The x-axis in figure 1 and 2 are not scales equally. In figure 2 a much higher $\lambda$-value is needed to have the coefficients approximately zero. Here is seems like year is goin more towards zero than cylinders and origin 3. This could be due to that year initially has a lower value.




```{r,echo=FALSE,results="hide"}
library(glmnet)
set.seed(4268)

x=model.matrix(mpg~.,ourAutoTrain)[,-1] #-1 to remove the intercept.
head(x)
y=ourAutoTrain$mpg

lambda=c(seq(from=5,to=0.1,length.out=150),0.01,0.0001) #Create a set of tuning parameters, adding low value to also see least squares fit
cv.out=cv.glmnet(x,y,alpha=1,nfolds=10,lambda=lambda, standardize=TRUE) #alpha=1 gives lasso, alpha=0 gives ridge

plot(cv.out)

```

##2b) Finding the optimal $\lambda$

* Q14: According to the description of the R-package glmnet, it "fits a generalized linear model via penalized maximum likelihood". cv.glmnet does k-fold cross-validation of the glmnet to produce the MSE for different $\lambda$-values.

* Q15: The plot shows how the MSE vary with the value of lambda. We want the MSE to be as small as possible, but we also want regularization to get a simpler model. Thus, two optimal $\lambda$-values are suggested by the plot, shown by a dotted line. The leftmost is that with lowest MSE value, and the other is the highest $\lambda$-values such that the MSE "is whithin one standard error of the minimum".

* Q16: 

```{r, echo=TRUE, eval=TRUE}

library(ISLR)
ourAuto=data.frame("mpg"=Auto$mpg,"cylinders"=factor(cut(Auto$cylinders,2)),
                   "displace"=Auto$displacement,"horsepower"=Auto$horsepower,
                   "weight"=Auto$weight,"acceleration"=Auto$acceleration, 
                   "year"=Auto$year,"origin"=as.factor(Auto$origin))

ntot=dim(ourAuto)[1]
set.seed(4268)
testids=sort(sample(1:ntot,ceiling(0.2*ntot),replace=FALSE))
ourAutoTrain=ourAuto[-testids,]
ourAutoTest=ourAuto[testids,]

library(glmnet)
set.seed(4268)

x=model.matrix(mpg~.,ourAutoTrain)[,-1] #-1 to remove the intercept.
y=ourAutoTrain$mpg

lambda=c(seq(from=5,to=0.1,length.out=150),0.01,0.0001) #Create a set of tuning parameters, adding low value to also see least squares fit
cv.out=cv.glmnet(x,y,alpha=1,nfolds=10,lambda=lambda, standardize=TRUE) #alpha=1 gives lasso, alpha=0 gives ridge
```


The $\lambda$-value that gives lowest MSE is
```{r, echo=TRUE, eval=TRUE}

cv.out$lambda.min
```

And the value that corresponds to 1se is
```{r, echo=TRUE, eval=TRUE}
L = cv.out$lambda.1se
cv.out$lambda.1se

```

We chose the last one to be the optimal $\lambda$, and use this for the next exercise.

##2c) Prediction

* Q17: 
Fitting a model with lasso using $\lambda = 0.01$ gives the following coefficients

```{r,echo=TRUE, eval=TRUE}
lasso = glmnet(x, y, alpha=1, lambda = L)

coef(lasso)
```

```{r,,echo=TRUE, eval=TRUE}
# 0 for cylinder, displace, horsepower, weight, acceleration, year, 0 for origin2 and 0 for origin3
newx=matrix(c(0,150,100,3000,10,82,0,0),nrow=1)
# then do the prediction
```

* Q18: Predicted value:

Thus the model is

$$Y = -21.5 -3.38x_{cylinders} + 0.031x_{displace} - 0.035x_{horsepower}-0.0061x_{weight}+0.12x_{acceleration} +0.78x_{year}+1.68x_{origin2}+2.83x_{origin3}$$

Q18:

The predicted mpg for the car with displace=150, horsepower=100, weight=3000, acceleration=10, year=82 and comes from Europe is
```{r, echo=FALSE}
x= 

P = predict(lasso, newx=matrix(c(0,150,100,3000,10,82,1,0),nrow=1), s=L)
P

```
