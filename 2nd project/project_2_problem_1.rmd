---
subtitle: "TMA4268 Statistical Learning V2018 - problem 3"
title: "Compulsory exercise 2: Group XX"
author: "NN1, NN2 and NN3"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: #3rd letter intentation hierarchy - uncomment html_document and comment pdf_document if neede
  # html_document
  pdf_document
editor_options: 
  chunk_output_type: console
---
  
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,results="hold",message = FALSE,warning=FALSE)
```
##1a)
  
* Q1: There are in total $2^d$ different linear regression models that can be fitted when we have d predictors. This comes from that we first have to fit all the d models that contain 1 predictor, then all $\binom{d}{2}=d(d-1)/2$ models that contain 2 predictors, all $\binom{d}{3}=d(d-1)(d-2)/3!$ that contain 3 predictors and so on up to $\binom{d}{d}=1$. Summing up all these combinations in addition to the "null model", the model where have no predictors, we get 
$$\sum_{k=0}^{d} \binom{d}{k} = ... = 2^d.$$
Here we used the binomial theorem $\sum_{k=0}^{d}\binom{d}{k}x^k=(1+x)^d$ with $x=1$.

* Q2: To choose the best model out of all possibilities we both need to consider the residual sum of squares (RSS) between models with equal amount of predictors and the BIC-criterion to choose among the best models for every number of predictors. This is presented in the followning algorithm:
1. For $k=0,1,2,...,d$: \newline
Fit all $\binom{d}{k}$ models that contain exactly k predictors. Then find the model with smallest RSS (largest $R^2$) and call it $\mathcal{M}_k$. For the case with $k=0$, we get $\mathcal{M}_0$, which simply predicts the sample mean for each observation. 
2. Select the best model among $\mathcal{M}_0, ...,\mathcal{M}_d$ using the BIC-criterion.

If we instead of BIC simply used $R^2$ as our base for selection of a model, we would get a good training error, but a poor test error. This is because the training error will decrease as more variables are included in the model, and we surely get a model with as many predictors as possible, which may cause overfitting. However, $R^2$ is good when comparing models with the same numbers of predictors, as in part 1 of the algorithm.


```{r,echo=FALSE,results="hide"}
library(ISLR)
ourAuto=data.frame("mpg"=Auto$mpg,"cylinders"=factor(cut(Auto$cylinders,2)),
                   "displace"=Auto$displacement,"horsepower"=Auto$horsepower,
                   "weight"=Auto$weight,"acceleration"=Auto$acceleration, 
                   "year"=Auto$year,"origin"=as.factor(Auto$origin))
colnames(ourAuto)
ntot=dim(ourAuto)[1]
ntot
set.seed(4268)
testids=sort(sample(1:ntot,ceiling(0.2*ntot),replace=FALSE))
ourAutoTrain=ourAuto[-testids,]
ourAutoTest=ourAuto[testids,]
n_train=dim(ourAutoTrain)[1]
n_test=dim(ourAutoTest)[1]
library(leaps)
res=regsubsets(mpg~.,nbest=1,data=ourAutoTrain)
sumres=summary(res)
sumres
plot(res,scale="bic")
sumres$bic
which.min(sumres$bic)
coef(res,id=which.min(sumres$bic))
```

##1b)

* Q3: The best model for each model complexity is chosen by taking the one with smallest RSS-value for a given number of predictors. The best model for computing miles pr gallon with two covariates is the one with `weight` and `year`. 


* Q4: To choose between the different models $\mathcal{M}_0$ to $\mathcal{M}_d$, we choose the one with smallest BIC-value. Here the BIC-value corresponding to the best model is $-520$. Looking at the black fields, we should choose the model with these seven covariates: intercept, cylinders, displace, horsepower, weight, year, origin2 and origin3. 

```{r, eval=TRUE,echo=TRUE}
library(ggplot2)
library(nortest)
#is it possible here to plot when we have seven covariates?
ourAutofit.lm=lm(mpg~.-acceleration,data=ourAutoTrain)
#summary(ourAutofit.lm)
MSE_train=mean((ourAutofit.lm$residuals)^2)

y_hat=predict.lm(ourAutofit.lm,ourAutoTest)
MSE_test=mean((y_hat-ourAutoTest$mpg)^2)

print("Anderson-Darling normality test")
ad.test(rstudent(ourAutofit.lm))
```
As seen in the summary, we get estimates for $\hat\beta$. The standard `weight`and `year`
* Q5: Use this model fit to predict new values for `ourAutoTest` and report the MSE.



##1c)

* Q6. Explain how $k$-fold cross-validation is performed. 
* Q7. Why may $k$-fold cross-validation be preferred to leave-one-out cross-validation? 

##1d)

* Q8. R-code for $10$-fold CV.

```{r,eval=TRUE,echo=TRUE,results="hold"}
library(caret)
library(leaps)
# and so on
```


* Q9. What is the optimal model complexity (number of parameters) in your regression?

```{r,echo=TRUE, eval=TRUE}
# MSE on test set
```

* Q10. Evaluate best model (or refer to Q4 and Q5).

##2a) Explain figures

* Q11: Which figure (1 or 2) corresponds to ridge and which figure corresponds to lasso? 
* Q12. Use the two figures and the above formulas to explain... 
* Q13. Can you use lasso and/or ridge regression to perform model selection similar to what you did in Problem 1? 

```{r,echo=FALSE,results="hide"}
library(glmnet)
set.seed(4268)

x=model.matrix(mpg~.,ourAutoTrain)[,-1] #-1 to remove the intercept.
head(x)
y=ourAutoTrain$mpg

lambda=c(seq(from=5,to=0.1,length.out=150),0.01,0.0001) #Create a set of tuning parameters, adding low value to also see least squares fit
cv.out=cv.glmnet(x,y,alpha=1,nfolds=10,lambda=lambda, standardize=TRUE) #alpha=1 gives lasso, alpha=0 gives ridge

plot(cv.out)

```

##2b) Finding the optimal $\lambda$

* Q14: Explain what the function `cv.glmnet` does. 
* Q15. Explain what we see in the above plot.
* Q16: Finding the optimal lambda:

```{r,echo=TRUE, eval=TRUE}
# need some R code here
```

##3c) Prediction

* Q17: Fit model, coefficients,...
```{r,echo=TRUE, eval=TRUE}
# fit the lasso
```

```{r,,echo=TRUE, eval=TRUE}
# 0 for cylinder, displace, horsepower, weight, acceleration, year, 0 for origin2 and 0 for origin3
newx=matrix(c(0,150,100,3000,10,82,0,0),nrow=1)
# then do the prediction
```

* Q18: Predicted value:

##3a)

* Q19: Fitting the specified `gam`

```{r,echo=TRUE, eval=TRUE}
library(gam)
# write R code
```

* Q20: The cubic spline basis.
