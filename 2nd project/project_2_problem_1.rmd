---
title: "Untitled"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,results="hold",message = FALSE,warning=FALSE)
```


##1a)
  
* Q1: There are in total $2^d$ different linear regression models that can be fitted when we have d predictors. This is due to the two choices we have for each covariate, either we can have it in our model or not. 



* Q2: To choose the best model out of all possibilities we both need to consider the residual sum of squares (RSS) between models with equal amount of predictors and the BIC-criterion to choose among the best models for every number of predictors. This is presented in the followning algorithm:
1. For $k=0,1,2,...,d$: \newline
Fit all $\binom{d}{k}$ models that contain exactly k predictors. Then find the model with smallest RSS (largest $R^2$) and call it $\mathcal{M}_k$. For the case with $k=0$, we get $\mathcal{M}_0$, which simply predicts the sample mean for each observation. 
2. Select the best model among $\mathcal{M}_0, ...,\mathcal{M}_d$ using the BIC-criterion.
If we instead of BIC simply used $R^2$ as our base for selecting a model, we would get a good training error, but a poor test error. This is because the training error will decrease as more variables are included in the model, and we surely get a model with as many predictors as possible, which may cause overfitting. However, $R^2$ is good when comparing models with the same numbers of predictors, as in part 1 of the algorithm.


```{r,echo=FALSE,results="hide"}
library(ISLR)
ourAuto=data.frame("mpg"=Auto$mpg,"cylinders"=factor(cut(Auto$cylinders,2)),
                   "displace"=Auto$displacement,"horsepower"=Auto$horsepower,
                   "weight"=Auto$weight,"acceleration"=Auto$acceleration, 
                   "year"=Auto$year,"origin"=as.factor(Auto$origin))
colnames(ourAuto)
ntot=dim(ourAuto)[1]
ntot
set.seed(4268)
testids=sort(sample(1:ntot,ceiling(0.2*ntot),replace=FALSE))
ourAutoTrain=ourAuto[-testids,]
ourAutoTest=ourAuto[testids,]
n_train=dim(ourAutoTrain)[1]
n_test=dim(ourAutoTest)[1]
library(leaps)
res=regsubsets(mpg~.,nbest=1,data=ourAutoTrain)
sumres=summary(res)
sumres
plot(res,scale="bic")
sumres$bic
which.min(sumres$bic)
coef(res,id=which.min(sumres$bic))
```

##1b)

* Q3: As explained in the theory, the best model for each model complexity is chosen by taking the one with smallest RSS-value for a given number of predictors. The best model for computing miles pr gallon with two covariates is the one with `weight` and `year`. 


* Q4: To choose between the different models $\mathcal{M}_0$ to $\mathcal{M}_d$, we choose the one with smallest BIC-value. Here the BIC-value corresponding to the best model is $-520$. Looking at the black fields, we should choose the model with these seven covariates: intercept, cylinders, displace, horsepower, weight, year, origin2 and origin3. 

```{r, eval=TRUE,echo=TRUE}
library(ggplot2)
library(nortest)
#Fit the model with all covariates except `acceleration`.
ourAutofit.lm=lm(mpg~.-acceleration,data=ourAutoTrain)
summary(ourAutofit.lm)
MSE_train=mean((ourAutofit.lm$residuals)^2)

#"Anderson-Darling normality test"
#ad.test(rstudent(ourAutofit.lm))
```

As seen in the summary, we get estimates for the $\hat\beta$'s. In general the estimates for $\hat\beta$'s vary between order $10e-3$ and $10e1$. The standard error is largest for the intercept. When looking at the p-values for the covariates, all are below the significane level, which means that we should believe there is a linear connection between `miles per gallon` and the covariates. Among the covariates, `weight`and `year` have the lowest p-values.
The MSE on `ourAutoTrain` is
```{r,eval=TRUE,echo=TRUE}
MSE_train
```

* Q5: The predicted new values for $\hat Y$ are computed by `predict` and we get the following value for the MSE for the test set. 
```{r,eval=TRUE,echo=TRUE}
y_hat=predict.lm(ourAutofit.lm,ourAutoTest)
MSE_test=mean((y_hat-ourAutoTest$mpg)^2)
MSE_test
```

##1c)

* Q6. Explain how $k$-fold cross-validation is performed. 
* Q7. Why may $k$-fold cross-validation be preferred to leave-one-out cross-validation? 


##1d)

* Q8: R code to perform a 10-fold cross-validation is used with the best subset method on `ourAutoTrain`.

```{r,eval=TRUE,echo=TRUE,results="hold"}
library(caret)
library(leaps)
#Predict function
predict.regsubset = function(object, newdata, id){
  form=as.formula(object$call[[2]])
  mat=model.matrix(form,newdata)
  coefi=coef(object,id=id)
  xvars=names(coefi)
  mat[,xvars]%*%coefi
}
#10-fold cross-validation
k=10
set.seed(4268)
folds=sample(1:k,nrow(ourAutoTrain),replace=TRUE)
cv.errors=matrix(NA,k,8,dimnames=list(NULL,paste(1:8)))

#Perform crossvalidation:
for (j in 1:k){
  best.fit=regsubsets(mpg~.,data=ourAutoTrain[folds!=j,],nvmax=8)
  for (i in 1:8){
    pred=predict.regsubset(best.fit,ourAutoTrain[folds==j,],id=i)
    cv.errors[j,i]=mean((ourAutoTrain$mpg[folds==j]-pred)^2)
  }
}
#Compute mean of cross-validation errors
mean.cv.errors=apply(cv.errors,2,mean)
mean.cv.errors
num_cov=which.min(mean.cv.errors)
par(mfrow=c(1,1))
plot(mean.cv.errors,type="b")
```

* Q9. The optimal model complexity is with 7 covariates, just as in 1b). We see this both from the plot and the list of averaged cross-validation errors. 

* Q10. Since we get the same model complexity from 10-fold cross-validation as with the best subset method, we get the same coefficients for the $\hat\beta$'s.
The new values for `ourAutoTest` are the same as in Q4 and Q5, as well as the MSE for the test set.


##2a) Explain figures

* Q11: Which figure (1 or 2) corresponds to ridge and which figure corresponds to lasso? 
* Q12. Use the two figures and the above formulas to explain... 
* Q13. Can you use lasso and/or ridge regression to perform model selection similar to what you did in Problem 1? 

```{r,echo=FALSE,results="hide"}
library(glmnet)
set.seed(4268)

x=model.matrix(mpg~.,ourAutoTrain)[,-1] #-1 to remove the intercept.
head(x)
y=ourAutoTrain$mpg

lambda=c(seq(from=5,to=0.1,length.out=150),0.01,0.0001) #Create a set of tuning parameters, adding low value to also see least squares fit
cv.out=cv.glmnet(x,y,alpha=1,nfolds=10,lambda=lambda, standardize=TRUE) #alpha=1 gives lasso, alpha=0 gives ridge

plot(cv.out)

```

##2b) Finding the optimal $\lambda$

* Q14: Explain what the function `cv.glmnet` does. 
* Q15. Explain what we see in the above plot.
* Q16: Finding the optimal lambda:

```{r,echo=TRUE, eval=TRUE}
# need some R code here
```

##3c) Prediction

* Q17: Fit model, coefficients,...
```{r,echo=TRUE, eval=TRUE}
# fit the lasso
```

```{r,,echo=TRUE, eval=TRUE}
# 0 for cylinder, displace, horsepower, weight, acceleration, year, 0 for origin2 and 0 for origin3
newx=matrix(c(0,150,100,3000,10,82,0,0),nrow=1)
# then do the prediction
```

* Q18: Predicted value:

##3a)

* Q19: Fitting the specified `gam`

```{r,echo=TRUE, eval=TRUE}
library(gam)
# write R code
```

* Q20: The cubic spline basis.
