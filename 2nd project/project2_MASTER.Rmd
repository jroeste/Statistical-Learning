---
title: 'Compulsory exercise 2: Group 19'
author: "Ida Marie Falnes, Astrid Langsrud and Julie Røste"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    df_print: paged
  pdf_document: default
subtitle: TMA4268 Statistical Learning V2018
editor_options:
  chunk_output_type: console
---
  
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,results="hold",message = FALSE,warning=FALSE)
```
##1a)
  
Q1: There are in total $2^d$ different linear regression models that can be fitted when we have d predictors. This is due to the two choices we have for each covariate, either we can have it in our model or not. 
Another way to come up with the same result is to look at the different models for each complexity. First we have to fit all the d models that contain 1 predictor, then all $\binom{d}{2}=d(d-1)/2$ models that contain 2 predictors, all $\binom{d}{3}=d(d-1)(d-2)/3!$ that contain 3 predictors and so on up to $\binom{d}{d}=1$. Summing up all these combinations in addition to the "null model", the model where have no predictors, we get 
$$\sum_{k=0}^{d} \binom{d}{k} = ... = 2^d.$$
Here we used the binomial theorem $\sum_{k=0}^{d}\binom{d}{k}x^k=(1+x)^d$ with $x=1$. 

Q2: To choose the best model out of all possibilities we both need to consider the residual sum of squares (RSS) between models with equal amount of predictors and the BIC-criterion to choose among the best models for every number of predictors. This is presented in the following algorithm:

1. For $k=0,1,2,...,d$: \newline
Fit all $\binom{d}{k}$ models that contain exactly k predictors. Then find the model with smallest RSS (largest $R^2$) and call it $\mathcal{M}_k$. For the case with $k=0$, we get $\mathcal{M}_0$, which simply predicts the sample mean for each observation.

2. Select the best model among $\mathcal{M}_0, ...,\mathcal{M}_d$ using the BIC-criterion, where $\text{BIC}=\frac{1}{n}(RSS+log(n)k{\hat\sigma}^2)$ with ${\hat\sigma}^2$ as an unbiased estimator for the variance.

If we instead of BIC simply used $R^2$ as our base for selecting a model, we would get a good training error, but a poor test error. This is because the training error will decrease as more variables are included in the model, and we surely get a model with as many predictors as possible, which may cause overfitting. However, $R^2$ is good when comparing models with the same numbers of predictors, as in part 1 of the algorithm.


```{r,echo=FALSE,results="hide"}
library(ISLR)
ourAuto=data.frame("mpg"=Auto$mpg,"cylinders"=factor(cut(Auto$cylinders,2)),
                   "displace"=Auto$displacement,"horsepower"=Auto$horsepower,
                   "weight"=Auto$weight,"acceleration"=Auto$acceleration, 
                   "year"=Auto$year,"origin"=as.factor(Auto$origin))
colnames(ourAuto)
ntot=dim(ourAuto)[1]
ntot
set.seed(4268)
testids=sort(sample(1:ntot,ceiling(0.2*ntot),replace=FALSE))
ourAutoTrain=ourAuto[-testids,]
ourAutoTest=ourAuto[testids,]
n_train=dim(ourAutoTrain)[1]
n_test=dim(ourAutoTest)[1]
library(leaps)
res=regsubsets(mpg~.,nbest=1,data=ourAutoTrain)
sumres=summary(res)
sumres
sumres$bic
which.min(sumres$bic)
coef(res,id=which.min(sumres$bic))
```

##1b)

Q3: As explained in the theory, the best model for each model complexity is chosen by taking the one with smallest RSS-value for a given number of predictors. The best model for computing miles pr gallon with two covariates is the one with `weight` and `year`. 


Q4: To choose between the different models $\mathcal{M}_0$ to $\mathcal{M}_d$, we choose the one with smallest BIC-value. Here the BIC-value corresponding to the best model is $-520$. Looking at the black fields, we should choose the model with these seven covariates: intercept, cylinders, displace, horsepower, weight, year, origin2 and origin3. 

```{r, eval=TRUE,echo=TRUE}
library(ggplot2)
library(nortest)
#Fit the model with all covariates except `acceleration`.
ourAutofit.lm=lm(mpg~.-acceleration,data=ourAutoTrain)
summary(ourAutofit.lm)
MSE_train=mean((ourAutofit.lm$residuals)^2)

ad.test(rstudent(ourAutofit.lm))
```

As seen in the summary, we get estimates for the $\hat\beta$'s. In general the estimates for $\hat\beta$'s vary between order $10^{-3}$ and 10. The standard error is largest for the intercept. When looking at the p-values for the covariates, all are below the significane level, which means that we should believe there is a linear dependence between `miles per gallon` and the covariates. Among the covariates, `weight` and `year` have the lowest p-values. Also, the $R^2$-value is relatively close to 1. ##Comment here on the Anderson Darling. ##
The MSE on `ourAutoTrain` is
```{r,eval=TRUE,echo=TRUE}
MSE_train
```

Q5: The predicted new values for $\hat Y$ are computed by `predict` and we get the following value for the MSE for the test set. 
```{r,eval=TRUE,echo=TRUE}
y_hat=predict.lm(ourAutofit.lm,ourAutoTest)
MSE_test=mean((y_hat-ourAutoTest$mpg)^2)
MSE_test
```
##1c)

Q6: $k$-fold cross validation is performed by randomly dividing the data set consisting of $n$ observations into $k$ approximately equally sized groups, or $folds$. One of these folds is left out and kept as a validation set, and the remaining $k-1$ folds are used to fit the model. Then the validation set is used to calculate the mean squared error ($MSE$) for the fitted model. The process is repated $k$ times, so an $MSE$ is calculated for each fold. If $n$ is a multiple of $k$, we have $n_k=n/k$ and the formula for $MSE_k$ is

$$
MSE_k=\frac{1}{n_k}\sum_{i \in C_k}(y_i-\hat y_i)^2,
$$

where $C_k$ denotes the held-out fold $k$, $\hat y_i$ is the fit obtained when holding out data from $C_k$, and $n_k$ is the number of observations in fold $k$. 

The final CV estimate is the average of all the $k$ MSEs:

$$
CV_k=\sum_{i=1}^k \frac{n_k}{n}MSE_k
$$

Q7. Why $k$-fold cross-validation may be preferred to leave-one-out cross-validation:
LOOCV is k-fold CV when $k=n$. When $k$ is substantially smaller than $n$, it follows that LOOCV usually is computationally more expensive than $k$-fold. This is because for the LOOCV $n$ different models is fitted, as opposed to only $k$ models in $k$-fold. In practice $k=5$ or $k=10$ is most common, as these values has turned out to give the best compromise between bias and variance, as we will discuss next.

Another reason for preferring $k$-fold CV over LOOCV comes from the bias-variance trade-off. LOOCV will have a low bias, but a rather high variance comapred to $k$-fold CV. This follows from the procedure of cross validation. One averages over model fits that consists of almost the same set of observations; only one observation differ for each model. This will cause high correlation between each model. When averaging over values, the higher correlated these are, the higher will the variance be. From this rationale, one can conclude that the models in $k$-fold cross-validation will be trained on less overlapping training data and will have less variance than the LOOCV.

##1d)

Q8: R code to perform a 10-fold cross-validation is used with the best subset method on `ourAutoTrain`.

```{r,eval=TRUE,echo=TRUE,results="hold"}
library(caret)
library(leaps)
#Predict function
predict.regsubset = function(object, newdata, id){
  form=as.formula(object$call[[2]])
  mat=model.matrix(form,newdata)
  coefi=coef(object,id=id)
  xvars=names(coefi)
  mat[,xvars]%*%coefi
}
#10-fold cross-validation
k=10
set.seed(4268)
folds=sample(1:k,nrow(ourAutoTrain),replace=TRUE)
cv.errors=matrix(NA,k,8,dimnames=list(NULL,paste(1:8)))

#Perform crossvalidation:
for (j in 1:k){
  best.fit=regsubsets(mpg~.,data=ourAutoTrain[folds!=j,],nvmax=8)
  for (i in 1:8){
    pred=predict.regsubset(best.fit,ourAutoTrain[folds==j,],id=i)
    cv.errors[j,i]=mean((ourAutoTrain$mpg[folds==j]-pred)^2)
  }
}
#Compute mean of cross-validation errors
mean.cv.errors=apply(cv.errors,2,mean)
mean.cv.errors
num_cov=which.min(mean.cv.errors)
par(mfrow=c(1,1))
plot(mean.cv.errors,type="b")
```
Q9. The optimal model complexity is with 7 covariates, just as in 1b), as we see from the list of averaged cross-validation errors with lowest value 10.562.

Q10. Since we get the same model complexity from 10-fold cross-validation as with the best subset method, we get the same coefficients for the $\hat\beta$'s.
The new values for `ourAutoTest` are the same as in Q4 and Q5, as well as the MSE for the test set.

##2a) Explain figures

Q11: The first figure corresponds to Lasso, and the second figure corresponds to Ridge regression. 

This can be concluded from the fact that the coefficient values in figure 1 becomes zero for large enough value of $\lambda$, which is only possible for Lasso and not for Ridge regression. In Ridge regression the coefficient values only approaches zero as $\lambda$ grows, which is corresponding to figure 2.  

Q12: The tuning parameter penalizes values of the estimated coefficients $\beta$, resulting in a reduction of the sum of the absolute values of the coefficients. When the value of $\lambda$ grows, this reduction also grows. 

This can be seen from the minimization formulas for Ridge regression and Lasso, as they include the terms $\lambda \sum_{j=1}^p \beta_j^2$ and $\lambda \sum_{j=1}^p |\beta_j|$ respectively. These terms favours $\beta$-values close to zero for minimization, and their fraction of the expressions depend on the value of $\lambda$. 

This can also be seen from the figures as these displays how the coefficient values vary with $\lambda$. The standardized coefficients tends towards zero as $\lambda$ increases. 

Increasing lambda will result in less flexibility of the model. As a consequence, increasing lamba means reducing variance and increasing bias. 

When $\lambda \rightarrow \infty$ the $\beta$-values will approach zero in Ridge regression and become zero in Lasso in order to minimize the model expressions. When $\lambda = 0$ both Ridge regression and Lasso will produce the same output as ordinary least squares estimate, as the penalty term is removed.

Q13:

For Lasso the coefficients eventually become zero when $\lambda$ is large enough, and thus the method results in model selection. On the other hand in Ridge regression the coefficients will not become zero, and thus it does not perform model selection.

In problem 1b BIC is used to compare models with different number of included variables. For each number of included variables the best model is found, and these are compared. From the figure in exerise 1b it is seen that weight, year and cylinders are the variables that are included in most of the models, indicating that these are important for the model. In figure 1 both year and cylinders need a relatively high $\lambda$-value to become zero. Weight has a very low coefficient value, and thus it is difficult to see when it becomes zero. In figure 2 it also seems like cylinder needs a reletively large $\lambda$-value to become zero. Here it is difficult to say much aboutt year since this has a lower coefficient value initially. However, it seems like the some of the same trends of relative importancy of the variables are captured in by the different methods.

The best model according to the method in 1b is that where only the acceleration variable is excluded.  On the other hand, for a high enough $\lambda$-value many variables are excluded in Lasso, and many variables approach zero in Ridge regression.This inticates that low $\lambda$-values gives models from Lasso and Ridge that is most similar to the best model found using BIC. 


```{r,echo=FALSE,results="hide"}
library(glmnet)
set.seed(4268)

x=model.matrix(mpg~.,ourAutoTrain)[,-1] #-1 to remove the intercept.
head(x)
y=ourAutoTrain$mpg

lambda=c(seq(from=5,to=0.1,length.out=150),0.01,0.0001) #Create a set of tuning parameters, adding low value to also see least squares fit
cv.out=cv.glmnet(x,y,alpha=1,nfolds=10,lambda=lambda, standardize=TRUE) #alpha=1 gives lasso, alpha=0 gives ridge

plot(cv.out)

```

##2b) Finding the optimal $\lambda$

Q14: According to the description of the R-package glmnet, it "fits a generalized linear model via penalized maximum likelihood". cv.glmnet does k-fold cross-validation of the glmnet to produce the MSE for different $\lambda$-values.

Q15: The plot shows how the MSE vary with the value of lambda. We want the MSE to be as small as possible, but we also want regularization to get a simpler model. Thus, two optimal $\lambda$-values are suggested by the plot, shown by a dotted line. The leftmost is that with lowest MSE value, and the other is 1se whichcorrespond to the highest $\lambda$-value such that the MSE not more than one standard error away from the minimum.

Q16: 

```{r, echo=TRUE, eval=TRUE}
library(glmnet)
set.seed(4268)

x=model.matrix(mpg~.,ourAutoTrain)[,-1] #-1 to remove the intercept.
y=ourAutoTrain$mpg

lambda=c(seq(from=5,to=0.1,length.out=150),0.01,0.0001) #Create a set of tuning parameters, adding low value to also see least squares fit
cv.out=cv.glmnet(x,y,alpha=1,nfolds=10,lambda=lambda, standardize=TRUE) #alpha=1 gives lasso, alpha=0 gives ridge
```


The $\lambda$-value that gives lowest MSE is
```{r, echo=TRUE, eval=TRUE}

cv.out$lambda.min
```

And the value that corresponds to 1se is
```{r, echo=TRUE, eval=TRUE}
L = cv.out$lambda.1se
cv.out$lambda.1se

```

We chose the last one to be the optimal $\lambda$, and we will use this for the next exercise.

##2c) Prediction

Q17: 
Fitting a model with lasso using $\lambda = 0.01$ gives the following coefficients

```{r,echo=TRUE, eval=TRUE}
lasso = glmnet(x, y, alpha=1, lambda = L)

coef(lasso)
```

Thus the model is

$$\hat{Y} = -21.5 -3.38x_{cylinders} + 0.031x_{displace} - 0.035x_{horsepower}-0.0061x_{weight}+0.12x_{acceleration} +0.78x_{year}+1.68x_{origin2}+2.83x_{origin3}$$

Q18:

The predicted mpg for the car with displace=150, horsepower=100, weight=3000, acceleration=10, year=82 and comes from Europe is
```{r, echo=FALSE}
x= 

P = predict(lasso, newx=matrix(c(0,150,100,3000,10,82,1,0),nrow=1), s=L)
P

```


##3a)

Q19: Fitting the specified `gam`: 

```{r,echo=TRUE, eval=TRUE}
library(gam)
library(ISLR)
library(GGally)
library(leaps)
library(glmnet)
library(ggplot2)
gamobject <- gam(mpg~bs(displace, knots = 290)+poly(horsepower,2)+
                   weight+s(acceleration, 3)+origin,data=ourAutoTrain)

par(mfrow=c(2,3))
plot(gamobject,se=TRUE,col="blue")

```
The resulting plots shows the fitted GAM with five components, in addition to the pointwise standard errors for each feature. In general, GAM-plots shows how the response, in our case the `mpg`, will vary when holding all but one of the variables constant. This is done for all five variables.

Upper left: A cubic spline in `displace` with one knot at 290. The plot shows that when increasing `displace` from 0 to 400, there is on average a decrease of `mpg`. Furthermore, the curve indicates that the response decreases faster for `displace`-values between 0 and 290. At the knot at 290, there seems to be a local minimum and the change in the response will slightly increase until 400, where further increasing engine displacement will lead to a more rapid decreasement of miles per gallon. Note that there are fewer observations from about 300 on the `displace`-axis, so error curves in this area is quite large. 

Top row, middle plot: A polynomial of degree 2 of `mpg` as a function of `horsepower`. The plot shows that an increase in horsepower will on average deacrease the use of miles per gallon. As in the previous plot, we also observe larger errors in the boundaries which can be explained by fewer observations in the ends of the curve. (almost no data points >200).

Upper right: Linear regression fit using `weight` as predictor. The fitted curve indicates that an increase in weight will on average lead to a decrease in `mpg`.

lower left: A smoothing spline with 3 degrees of freedom. Most of the data points lies in the area between 13 and 20. Here, the curve indicates that for increased acceleration of the car, the `mpg` will decrease. Few observations in the endpoints lead to large error curves here. 

Lower right: Step function. We have observed more american vehicles, and the variance is much smaller for the these cars. On average the European cars have less `mpg` than Americans, that in turn use slighty less than the Japanese cars, according to our data.

Q20: We have an order $M=4$ spline with $K=1$ knot. In general we will have $M+K-1=4$ basis functions. In our case, the basis the cubic spline is $X,X^2,X^3,(x-c_1)_+^3$. The truncated power function is

$$
(x-290)^3= \begin{cases}(x-290)^3, x>290 \\
0, \text{otherwise}
\end{cases}
$$
for the knot $c_1=290$.
