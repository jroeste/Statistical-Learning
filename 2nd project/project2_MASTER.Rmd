---
subtitle: "TMA4268 Statistical Learning V2018"
title: "Compulsory exercise 2: Group XX"
author: "NN1, NN2 and NN3"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: #3rd letter intentation hierarchy - uncomment html_document and comment pdf_document if neede
  # html_document
  pdf_document
---
  
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,results="hold",message = FALSE,warning=FALSE)
```
##1a)
  
* Q1: How many possible models?
* Q2: How to choose best model

```{r,echo=FALSE,results="hide"}
library(ISLR)
ourAuto=data.frame("mpg"=Auto$mpg,"cylinders"=factor(cut(Auto$cylinders,2)),
                   "displace"=Auto$displacement,"horsepower"=Auto$horsepower,
                   "weight"=Auto$weight,"acceleration"=Auto$acceleration, 
                   "year"=Auto$year,"origin"=as.factor(Auto$origin))
colnames(ourAuto)
ntot=dim(ourAuto)[1]
ntot
set.seed(4268)
testids=sort(sample(1:ntot,ceiling(0.2*ntot),replace=FALSE))
ourAutoTrain=ourAuto[-testids,]
ourAutoTest=ourAuto[testids,]
library(leaps)
res=regsubsets(mpg~.,nbest=1,data=ourAutoTrain)
sumres=summary(res)
sumres
plot(res,scale="bic")
sumres$bic
which.min(sumres$bic)
coef(res,id=which.min(sumres$bic))
```

##1b)

* Q3: How best model chosen for each model complexity + best for 2 covariates.
* Q4: Choose model and evaluate.

```{r, eval=TRUE,echo=TRUE}
# R code to fit your final model.
```

* Q5: Use this model fit to predict new values for `ourAutoTest` and report the MSE. 

## c) Cross-validation [1 point]

* Q6: $k$-fold cross validation is performed by randomly dividing the data set consisting of $n$ observations into $k$ approximately equally sized groups, or $folds$. The first fold is left out and kept as a validation set, and the remaining $k-1$ folds are used to fit the model. Then the validation set is used to calculate the $MSE$. The process is repated $k$ times, so the $MSE_i$ is calculated for fold $i=1,...,k$. If $n$ is a multiple of $k$, we have $n_k=n/k$ and the formula for $MSE_k$ is

$$
MSE_k=\frac{1}{n_k}\sum_{i \in C_k}(y_i-\hat y_i)^2,
$$

where $C_k$ denotes the held-out fold $k$, $\hat y_i$ is the fit obtained when holding out data from $C_k$, and $n_k$ is the number of observations in fold $k$. 

The final CV estimate is the average of all the $k$ MSEs:

$$
CV_k=\frac{1}{n}\sum_{i=j}^k n_k MSE_k
$$

* Q7. Why may $k$-fold cross-validation be preferred to leave-one-out cross-validation?
LOOCV is k-fold CV for $k=n$, so LOOCV is usually computationally more expensive than $k$-fold when $k$ is substantially smaller than $n$. This is because for the LOOCV $n$ different models is fitted, as opposed to $k=5$ or $k=10$ that is most common $k$-fold CV in practice. 

Another reason for preferring $k$-fold CV over LOOCV comes from the bias-variance trade-off. LOOCV will have a low bias, but a rather high variance comapred to $k$-fold CV. This follows from the procedure of cross validation. One averages over model fits that consists of almost the same set of observations; only one observation differ for each model. This will cause high correlation between each model. When performing LOOCV, one is in general averaging over the models, and the higher correlated these are, the higher will the variance be. From this rationale, the models in $k$-fold cross-validation will be trained on less overlapping training data and will have less variance than the LOOCV.

##1d@9

* Q8. R-code for $10$-fold CV.

```{r,eval=TRUE,echo=TRUE,results="hold"}
library(caret)
library(leaps)
# and so on
```


* Q9. What is the optimal model complexity (number of parameters) in your regression?

```{r,echo=TRUE, eval=TRUE}
# MSE on test set
```

* Q10. Evaluate best model (or refer to Q4 and Q5).

##2a) Explain figures

* Q11: Which figure (1 or 2) corresponds to ridge and which figure corresponds to lasso? 
* Q12. Use the two figures and the above formulas to explain... 
* Q13. Can you use lasso and/or ridge regression to perform model selection similar to what you did in Problem 1? 

```{r,echo=FALSE,results="hide"}
library(glmnet)
set.seed(4268)

x=model.matrix(mpg~.,ourAutoTrain)[,-1] #-1 to remove the intercept.
head(x)
y=ourAutoTrain$mpg

lambda=c(seq(from=5,to=0.1,length.out=150),0.01,0.0001) #Create a set of tuning parameters, adding low value to also see least squares fit
cv.out=cv.glmnet(x,y,alpha=1,nfolds=10,lambda=lambda, standardize=TRUE) #alpha=1 gives lasso, alpha=0 gives ridge

plot(cv.out)

```

##2b) Finding the optimal $\lambda$

* Q14: Explain what the function `cv.glmnet` does. 
* Q15. Explain what we see in the above plot.
* Q16: Finding the optimal lambda:

```{r,echo=TRUE, eval=TRUE}
# need some R code here
```

##2c) Prediction

* Q17: Fit model, coefficients,...
```{r,echo=TRUE, eval=TRUE}
# fit the lasso
```

```{r,,echo=TRUE, eval=TRUE}
# 0 for cylinder, displace, horsepower, weight, acceleration, year, 0 for origin2 and 0 for origin3
newx=matrix(c(0,150,100,3000,10,82,0,0),nrow=1)
# then do the prediction
```

* Q18: Predicted value:

##3a)

* Q19: Fitting the specified `gam`

```{r,echo=TRUE, eval=TRUE}
library(gam)
library(ISLR)
library(GGally)
library(leaps)
library(glmnet)
library(ggplot2)

library(ISLR)
ourAuto=data.frame("mpg"=Auto$mpg,"cylinders"=factor(cut(Auto$cylinders,2)),
                   "displace"=Auto$displacement,"horsepower"=Auto$horsepower,
                   "weight"=Auto$weight,"acceleration"=Auto$acceleration, 
                   "year"=Auto$year,"origin"=as.factor(Auto$origin))
colnames(ourAuto)
ntot=dim(ourAuto)[1]
set.seed(4268)
testids=sort(sample(1:ntot,ceiling(0.2*ntot),replace=FALSE))
ourAutoTrain=ourAuto[-testids,]
ourAutoTest=ourAuto[testids,]
###i den endelige innleveringsfilen trenger vi bare kodesnutten herfra og ned###

library(plyr)
ourAutoTrain$origin=mapvalues(ourAutoTrain$origin, from = c("1", "2","3"),
                              to = c('Am','Eu','Jap'))
gamobject <- gam(mpg~bs(displace, knots = 290)+poly(horsepower,2)+
                   weight+s(acceleration, 3)+origin,data=ourAutoTrain)

par(mfrow=c(2,3))
plot(gamobject,se=TRUE,col="blue")

```
The resulting plots shows the fitted GAM with five components, in addition to the pointwise standard errors for each feature. In general, GAM-plots shows how the response, in our case the `mgp`, will vary when holding all but one of the variables constant. This is done for all five variables.

Upper left: A cubic spline in `displace` with one knot at 290. The plot shows that when increasing `displace` from 0 to 400, there is on average a decrease of `mpg`. Furthermore, the curve indicates that the response decreases faster for `displace`-values between 0 and 290. At the knot at 290, there seems to be a local minimum and the change in the response is smaller until 400, where further increasing engine displacement will lead to a more rapid decreasement of miles per gallon. Note that the error curves in both ends are quite large. A reason for this is fewer data points in these areas.

Top, in the middle: A polynomial of degree 2 of `mpg` as a function of `horsepower`. The plot shows that an increase in horsepower will on average deacrease the use of miles per gallon. As in the previous plot, we also observe larger errors in the boundaries which can be explained by fewer observations in the ends of the curve. (almost no data points >200).

Upper right: Linear regression fit using `weight` as predictor. The fitted curve indicates that an increase in weight will on average lead to a decrease in `mpg`.

lower left: A smoothing spline with 3 degrees of freedom. Most of the data points lies in the area between 13 and 20. Here, the curve indicates that for increased acceleration of the car, the `mpg` will decrease. Few observations in the endpoints lead to large error curves here. 

Lower right: Step function. We have observed more american vehicles, and hence the variance is much smaller for the american cars. On average the European cars have less `mpg` than American that in turn use slighty less than the Japanese cars.

* Q20: We have an order $M=4$ spline with $K=1$ knot. In general we will have $M+K-1=4$ basis functions. In our case, the basis the cubic spline is $X,X^2,X^3,(x-c_1)_+^3$. The truncated power function is

$$
(x-290)^3= \begin{cases}(x-290)^3, x>290 \\
0, \text{otherwise}
\end{cases}
$$
for the knot $c_1=290$.
