---
title: 'Compulsory exercise 2: Group XX'
author: "NN1, NN2 and NN3"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    df_print: paged
subtitle: TMA4268 Statistical Learning V2018 - problem 3
editor_options:
  chunk_output_type: console
---

  
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,results="hold",message = FALSE,warning=FALSE)
```
##1a)
  
* Q1: We must consider $2^d$ different models, since $d + d(d+1)/2 + d(d-1)(d-2)/3! + ... + 1 = 2^d $, by binomial theorem. 
* Q2: How to choose best modelx|

```{r,echo=FALSE,results="hide"}
library(ISLR)
ourAuto=data.frame("mpg"=Auto$mpg,"cylinders"=factor(cut(Auto$cylinders,2)),
                   "displace"=Auto$displacement,"horsepower"=Auto$horsepower,
                   "weight"=Auto$weight,"acceleration"=Auto$acceleration, 
                   "year"=Auto$year,"origin"=as.factor(Auto$origin))
colnames(ourAuto)
ntot=dim(ourAuto)[1]
ntot
set.seed(4268)
testids=sort(sample(1:ntot,ceiling(0.2*ntot),replace=FALSE))
ourAutoTrain=ourAuto[-testids,]
ourAutoTest=ourAuto[testids,]
library(leaps)
res=regsubsets(mpg~.,nbest=1,data=ourAutoTrain)
sumres=summary(res)
sumres
plot(res,scale="bic")
sumres$bic
which.min(sumres$bic)
coef(res,id=which.min(sumres$bic))
```

##1b)

* Q3: How best model chosen for each model complexity + best for 2 covariates.
* Q4: Choose model and evaluate.

```{r, eval=TRUE,echo=TRUE}
# R code to fit your final model.
```

* Q5: Use this model fit to predict new values for `ourAutoTest` and report the MSE. 


##1c)

* Q6. Explain how $k$-fold cross-validation is performed. 
* Q7. Why may $k$-fold cross-validation be preferred to leave-one-out cross-validation? 

##1d@9

* Q8. R-code for $10$-fold CV.

```{r,eval=TRUE,echo=TRUE,results="hold"}
library(caret)
library(leaps)
# and so on
```


* Q9. What is the optimal model complexity (number of parameters) in your regression?

```{r,echo=TRUE, eval=TRUE}
# MSE on test set
```

* Q10. Evaluate best model (or refer to Q4 and Q5).

##2a) Explain figures

* Q11: The first figure corresponds to Lasso, and the second figure corresponds to Ridge regression. 

This can be concluded from the fact that the coefficient values in the first figure becomes zero for large enough value of $\lambda$, which is only possible for Lasso and not for Ridge regression. In Ridge regression the coefficient values only approaches zero as $\lambda$ grows, which is corresponding to figure 2.  


* Q12. The tuning parameter penalizes values of the estimated coefficients $\beta$, resulting in a reduction of the sum of the coefficient absolute values. This reduction increases with the value of $\lambda$.

This can be seen from the formulas, as they include the terms $\lambda \sum_{j=1}^p \beta_j^2$ and $\lambda \sum_{j=1}^p |\beta_j|$ respectively in ridge regression and lasso. These terms favours $\beta$-values close to zero, as the $\beta$-values should be chosen to minimize the expressions for the two models. 

This can also be seen from the figures as these displays how the coefficient values vary with $\lambda$. The standardized coefficients tends towards zero as $\lambda$ grows. 

Increasing lambda will result in less flexibility of the model. As a consequence, increasing lamba means reducing variance and increasing bias. 

When $\lambda \rightarrow \infty$ the $\beta$-values will approach zero in Ridge regression and become zero in Lasso in order to minimize the model expressions. When $\lambda = 0$ both Ridge regression and Lasso will produce the same output as ordinary least squares estimate, as the penalty term is removed.

* Q13. Can you use lasso and/or ridge regression to perform model selection similar to what you did in Problem 1? 

```{r,echo=FALSE,results="hide"}
library(glmnet)
set.seed(4268)

x=model.matrix(mpg~.,ourAutoTrain)[,-1] #-1 to remove the intercept.
head(x)
y=ourAutoTrain$mpg

lambda=c(seq(from=5,to=0.1,length.out=150),0.01,0.0001) #Create a set of tuning parameters, adding low value to also see least squares fit
cv.out=cv.glmnet(x,y,alpha=1,nfolds=10,lambda=lambda, standardize=TRUE) #alpha=1 gives lasso, alpha=0 gives ridge

plot(cv.out)

```

##2b) Finding the optimal $\lambda$

* Q14: According to the description of the R-package glmnet, it "fits a generalized linear model via penalized maximum likelihood". cv.glmnet does k-fold cross-validation of the glmnet to produce the MSE for different $\lambda$-values.

* Q15: The plot shows how the MSE vary with the value of lambda. We want the MSE to be as small as possible, but we also want regularization to get a simpler model. Thus, two optimal $\lambda$-values are suggested by the plot, shown by a dotted line. The leftmost is that with lowest MSE value, and the other is the highest $\lambda$-values such that the MSE "is whithin one standard error of the minimum".

* Q16: 

```{r, include=FALSE}

library(ISLR)
ourAuto=data.frame("mpg"=Auto$mpg,"cylinders"=factor(cut(Auto$cylinders,2)),
                   "displace"=Auto$displacement,"horsepower"=Auto$horsepower,
                   "weight"=Auto$weight,"acceleration"=Auto$acceleration, 
                   "year"=Auto$year,"origin"=as.factor(Auto$origin))

ntot=dim(ourAuto)[1]
set.seed(4268)
testids=sort(sample(1:ntot,ceiling(0.2*ntot),replace=FALSE))
ourAutoTrain=ourAuto[-testids,]
ourAutoTest=ourAuto[testids,]

library(glmnet)
set.seed(4268)

x=model.matrix(mpg~.,ourAutoTrain)[,-1] #-1 to remove the intercept.
y=ourAutoTrain$mpg

lambda=c(seq(from=5,to=0.1,length.out=150),0.01,0.0001) #Create a set of tuning parameters, adding low value to also see least squares fit
cv.out=cv.glmnet(x,y,alpha=1,nfolds=10,lambda=lambda, standardize=TRUE) #alpha=1 gives lasso, alpha=0 gives ridge
```


The $\lambda$-value that gives lowest MSE is
```{r, echo=FALSE}

cv.out$lambda.min
```

And the value that corresponds to 1se is
```{r, echo=FALSE}
L = cv.out$lambda.1se
cv.out$lambda.1se

```

We chose the last one to be the optimal $\lambda$, and use this for the next exercise.

```{r,echo=TRUE, eval=TRUE}
# need some R code here
```

##2c) Prediction

* Q17: Fit model, coefficients,...

Fitting a model with lasso using $\lambda = 0.01$ gives the following coefficients

```{r, echo=FALSE}
lasso = glmnet(x, y, alpha=1, lambda = L)

coef(lasso)
```

Thus the model is

$$Y = -21.5 -3.38x_{cylinders} + 0.031x_{displace} - 0.035x_{horsepower}-0.0061x_{weight}+0.12x_{acceleration} +0.78x_{year}+1.68x_{origin2}+2.83x_{origin3}$$

* Q18: Predicted value:

The predicted mpg for the car with displace=150, horsepower=100, weight=3000, acceleration=10, year=82 and comes from Europe is
```{r, echo=FALSE}
x= 

P = predict(lasso, newx=matrix(c(0,150,100,3000,10,82,1,0),nrow=1), s=L)
P

```

##3a)

* Q19: Fitting the specified `gam`

```{r,echo=TRUE, eval=TRUE}
library(gam)
# write R code
```

* Q20: The cubic spline basis.