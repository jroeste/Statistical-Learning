---
title: "Statistical_learning_1"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(warnings=FALSE)
knitr::opts_chunk$set(message =FALSE)
```

##Problem 1
###Task a) Training and test MSE
In problem 1 we consider a regression problem where the true underlying curve is $$f(x)=-x + x^2 + x^3$$. 

The K nearest neighbour method is used to compute regression models. The aim of this problem is to compare different K values for this regression.

Figure 2 shows that the $M=1000$ estimated models are spread over a relatively large range when $K=1$.This spread shrinkes while K increases. With $K=10$ this spread is approximately halved. However, as the K-value increases, the mean of the M repetitions deviates more from the underlying function. Especially around the endpoints of the x-values, the regression lines for high K-values become horisontal, which does not correspond to $f(x)=1+x^2+x^3$.

These observations can be explained by the variyng flexibility for different K-values. When the K-value is low, the flexibility is high, so the fitted model with $K=1$ vary a lot for different training data sets. On the other hand, a high K-value corresponds to low flexibility, which can resuat in misleading fitted models in the sense that the estimated mean deviates from the true mean.

Figure 3 shows how the trainMSE and testMSE varies with K. The mean-squared errors for the training set is stricktly increasing for increasing K. This is because for a lower K-value, the fitted model more precicely corresponds to the data points in the training set. However, this does not mean that the best model is with $K=1$, since some of the trends captured from the training data set might be due to randomness and not the actual trends in the underlying curve.

To get an indication of which K-value is the best to choose, one has to compute the MSE-value for a test set of other data points than in the training set. For the test set the mean-squared errors has a minimum value for K around $K=3$ to $K=5$ and increases for higher and lower values. This indicates that choosing a K-value from 3 to 5 would give the best model fit. 


###Task b)
The goal is to reduce the total error of the regression model, which is


$$\text{E}[(Y - \hat{f}(x_0))^2]=\text{Var}(\varepsilon) +  \text{Var}[\hat{f}(x_0)]+[\text{Bias}(\hat{f}(x_0))]^2$$


The variance for a given x-value is calculated as the sample variance of the predicted values from the M regression models. The bias is calculated from the difference of the mean of the predicted values and the true response value. 

When the value of K decreases the model gets more flexible, and hence the squared bias decreases. On the other hand the variance increases when the K value decreases. The irreducible error is constant, and is independent of K. 

In the figure the total error has minimum point around K value 3-5, which is in agreement with what we found in exercise a). 

##Problem 2
###Task a)
In problem 2 we study a data set from Fermingham heart study. We want to model systolic blood pressure based on seven variables. 

Equation for fitted model: 

$$Y = -1.103 \cdot 10^{-1} - 2.989 \cdot 10^{-4}x_{SEX} + 2.378 \cdot 10^{-4}x_{AGE} - 2.504 \cdot 10^{-4}x_{CURSMOKE} + 3.087 \cdot 10^{-4}x_{BMI} + 9.288 \cdot 10^{-6}x_{TOTCHOL} + 5.469 \cdot 10^{-3}x_{BPMEDS}$$

Explanation of the summary-output:

`Estimate`: Gives the estimated numerical values of the coefficients in the equation of the linear regression of the data. The first value corresponds to the estimate of the intercept. Grapically this is the point where the linear regression line intercepts the y-axis. Practically this gives the output value when all input variables are zero. 

`Std.Error`: Gives the square root of the estimated variance.

`t value`: The t statistic is used to test a hypothesis. In this case we consider the following null hypothesis:



$$H_0: \beta = 0 \textrm{, there is no actual relationship between the parameter and the response}$$
The t value is number of standard deviations that the estimated beta is away from the null hypothesis $$t = \frac{\hat\beta - \beta}{SE(\hat\beta)}$$. 

`Pr(>|t|)`: Measure of p-value for the hypothesis test. The number Pr(>|t|) represents the probability of observing data resulting in an estimated $\beta$-value with absolute value equal to or larger than the one estimated for this data set, given that the null hypothesis is true. 

`Residual standard error`:

Estimate of the standard deviation of the error $\epsilon$. It is computed as

$$RSE = \sqrt{RSS/(n-p-2)}$$
where n is the number of observations, p is the number of distinct predictors and $RSS=\sum_{i=1}^{n}(y_i - \hat y_i)^2$

`F-statistic`:

Formula:

$$F = \frac{(TSS-RSS)/p}{RSS/(n-p-1)}$$

TSS is the total sum of squares, which equals $\sum_{}^{}(y_i - \bar y)^2$.

The value of the F-statistic is a result of performing the hypothesis test with null hypothesis:

$$H_0: \beta_1 = \beta_2 = ... = \beta_p = 0$$

If the null hypothesis is correct, the F-statistic has expected value 1. If at least one of the parameters has real value above zero, the expected value of F is greater than 1. The F-statistic can thus be used to compare models. A model with higher F-statistic catches the correlations in the population better. 

###Task b)
We fit a regression model to the data set with -1/sqrt(SYSBP) as response, where SYSBP is the systolic blood pressure. 

The proportion of variability explained by this model is 0.2494, given by the $R^2$-value. This means that about one forth of the variability in the `-1/sqrt(SYSBP)` data is explained by `sex`, `age`, `current smoking`, `BMI`, `total cholesterol` and `anti-hypersensitive medication`. Thus, about three forths of the veriability comes from other things that we haven't measured.

The figure below shows a plot of the residuals vs the fitted values. 
```{r echo = FALSE}
library(ggplot2)
data = read.table("https://www.math.ntnu.no/emner/TMA4268/2018v/data/SYSBPreg3uid.txt")
modelA=lm(-1/sqrt(SYSBP) ~ .,data = data)


# residuls vs fitted
ggplot(modelA, aes(.fitted, .resid)) + geom_point(pch = 21) + 
  geom_hline(yintercept = 0, linetype = "dashed") + 
  geom_smooth(se = FALSE, col = "red", size = 0.5, method = "loess") + 
  labs(x = "Fitted values", y = "Residuals", title = "Fitted values vs. residuals", subtitle =deparse(modelA$call))

```

The figure shows that most of the residuals lie between 0.015 and -0.015. The red line represents the average residual value at each fitted value. This shows that the residuals are mostly sentered around zero. Ideally the residuals should be small, symetrically distributed, and not show any patterns with the size of the fitted values. For large fitted values the average seems to decrease. However this could be due to randomness since there are few measured values of that size. Thus, it seems like the residuals are relatively independent of the fitted values. 

The next figure is a Q-Q-plot of the standarized residuals.

```{r echo=FALSE}

# qq-plot of residuals
ggplot(modelA, aes(sample = .stdresid)) +
  stat_qq(pch = 19) + 
  geom_abline(intercept = 0, slope = 1, linetype = "dotted") +
  labs(x = "Theoretical quantiles", y = "Standardized residuals", title = "Normal Q-Q", subtitle = deparse(modelA$call))


```

Here the residuals follows a straight line. This means that the residuals are normally distributed.

 

```{r, echo = FALSE}
# normality test
library(nortest) 
ad.test(rstudent(modelA))
```

The Anderson-Darling normality test for this model gives a p-value of 0.8959, which is in agreement with the assumption that the residuals come froma normal distribution. 

Over all it seems likely that the residuals come from a normal distribution, which indicates that the linear regression model works well for the data. However, due to a relatively low $R^2$-value, there are probably many unknown factors influencing the blood pressure. 

Now we want to compare this model to a similar linear regression model with respones variable SYSBP rather than -1/sqrt(SYSBP). An advantage of using SYSBP directly as response, is that the regression model will be somewhat easier to interpret.

The following is a "fitted values vs residuals"-plot, a Q-Q-plot and the Anderson-Darling normality test for this model. 



```{r echo=FALSE}

library(ggplot2)
data = read.table("https://www.math.ntnu.no/emner/TMA4268/2018v/data/SYSBPreg3uid.txt")
modelB=lm(SYSBP ~ .,data = data)

# residuls vs fitted
ggplot(modelB, aes(.fitted, .resid)) + geom_point(pch = 21) + 
  geom_hline(yintercept = 0, linetype = "dashed") + 
  geom_smooth(se = FALSE, col = "red", size = 0.5, method = "loess") + 
  labs(x = "Fitted values", y = "Residuals", title = "Fitted values vs. residuals", subtitle = deparse(modelB$call))

# qq-plot of residuals
ggplot(modelB, aes(sample = .stdresid)) +
  stat_qq(pch = 19) + 
  geom_abline(intercept = 0, slope = 1, linetype = "dotted") +
  labs(x = "Theoretical quantiles", y = "Standardized residuals", title = "Normal Q-Q", subtitle = deparse(modelB$call))

# normality test
library(nortest) 
ad.test(rstudent(modelB))

```

The "fitted values vs residuals"-plot for this is quite similar to that of the previous model. 

In this Q-Q-plot more of the points deviate from the straight line. Also, the p-value of the normality test for this moedl is very low. Hence, the residuals in this model is less likely to come from a normal distribution, indicating that linear regression is not as suitable with SYSBP as response. 

Over all it seems like the response variable -1/sqrt(SYSBP) has a more linear correlation with the input parameters than SYSBP. Therefore we would perfer to use the first regression model.

###Task b) K-nearest neighbor classifier 

The equation 

$$
P(Y=1|X=x_0) = \frac{1}{K}\sum_{i\in \mathcal{N_0}}I(y_i=1).
$$

gives the probability that a test observation with predictor values $\textbf{x_0}$ has response value $Y=1$. This probability is given as the proportion of the K nearest points in the training data that are in class 1. The equation includes an indicator function $I(y_i = 1)$, which equals 1 if $y_i = 1$ and 0 if $y_i \neq 1$. In this way it sums up the number of neighboors that are in class 1. 

K-nearest neighboor is used to classify the wines in the test set. The confusion table, sensitivity and specificity for KNN with $K=3$ are given below:

Confusion table:

```{r, echo=FALSE}

library(class)
knn.train = train
knn.test = test

knn1.wine = knn(train = knn.train[,-1], test = knn.test[,-1], cl = train$y, k=3)
tknn3 = table(knn1.wine, test$y)
tknn3

```
Sensitivity:
```{r, echo=FALSE}

sens_3 = tknn3[2,2]/(tknn3[1,2]+tknn3[2,2])
sens_3

```

Specificity:
```{r, echo=FALSE}
spec_3 = tknn3[1,1]/(tknn3[1,1]+tknn3[2,1])
spec_3

```
These results show that the classifier is not able to classify all test observations correctly. However, considering the overlap of the two classes seen in the plot in the exercise text, there is some irreducible error present. Thus, the classifier might do well.  


With $K=9$ we got the following results:

Confusion table:
```{r, echo=FALSE}

knn1.wine = knn(train = knn.train[,-1], test = knn.test[,-1], cl = train$y, k=9)
tknn9 = table(knn1.wine, test$y)
tknn9
```

Sensitivity:
```{r, echo=FALSE}
sens_9 = tknn9[2,2]/(tknn9[1,2]+tknn9[2,2])
sens_9

```

Specificity:
```{r, echo=FALSE}
spec_9 = tknn9[1,1]/(tknn9[1,1]+tknn9[2,1])
spec_9

```

For K=9 the sensitivity is lower that for K=3, indicating that the classifier with K=3 is better. However, the data set is not so large, so this difference is due to a few single wines. This means that we cannot be certain of which of the K-values that are best. 

Because of the bias-variance trade-off we don't choose the K-value as low or high as possible. 