---
title: "Statistisk l√¶ring, prosjekt 2 kladd"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Q11:

The first figure corresponds to Lasso, and the second figure corresponds to Ridge regression. 

This can be concluded from the fact that the coefficient values in the first figure becomes zero for large enough value of $\lambda$, which is only possible for Lasso and not for Ridge regression. In Ridge regression the coefficient values only approaches zero as $\lambda$ grows, which is corresponding to figure 2.  

Q12: The tuning parameter penalizes values of the estimated coefficients $\beta$, resulting in a reduction of the sum of the coefficient absolute values. This reduction increases with the value of $\lambda$.

This can be seen from the formulas, as they include the terms $\lambda \sum_{j=1}^p \beta_j^2$ and $\lambda \sum_{j=1}^p |\beta_j|$ respectively in ridge regression and lasso. These terms favours $\beta$-values close to zero, as the $\beta$-values should be chosen to minimize the expressions for the two models. 

This can also be seen from the figures as these displays how the coefficient values vary with $\lambda$. The standardized coefficients tends towards zero as $\lambda$ grows. 

Increasing lambda will result in less flexibility of the model. As a consequence, increasing lamba means reducing variance and increasing bias. 

When $\lambda \rightarrow \infty$ the $\beta$-values will approach zero in Ridge regression and become zero in Lasso in order to minimize the model expressions. When $\lambda = 0$ both Ridge regression and Lasso will produce the same output as ordinary least squares estimate, as the penalty term is removed.

Q14: According to the description of the R-package glmnet, it "fits a generalized linear model via penalized maximum likelihood". cv.glmnet does k-fold cross-validation of the glmnet to produce the MSE for different $\lambda$-values. 

Q15: The plot shows how the MSE vary with the value of lambda. We want the MSE to be as small as possible, but we also want regularization to get a simpler model. Thus, two optimal $\lambda$-values are suggested by the plot, shown by a dotted line. The leftmost is that with lowest MSE value, and the other is the highest $\lambda$-values such that the MSE "is whithin one standard error of the minimum".

Q16: 

```{r, include=FALSE}

library(ISLR)
ourAuto=data.frame("mpg"=Auto$mpg,"cylinders"=factor(cut(Auto$cylinders,2)),
                   "displace"=Auto$displacement,"horsepower"=Auto$horsepower,
                   "weight"=Auto$weight,"acceleration"=Auto$acceleration, 
                   "year"=Auto$year,"origin"=as.factor(Auto$origin))

ntot=dim(ourAuto)[1]
set.seed(4268)
testids=sort(sample(1:ntot,ceiling(0.2*ntot),replace=FALSE))
ourAutoTrain=ourAuto[-testids,]
ourAutoTest=ourAuto[testids,]

library(glmnet)
set.seed(4268)

x=model.matrix(mpg~.,ourAutoTrain)[,-1] #-1 to remove the intercept.
y=ourAutoTrain$mpg

lambda=c(seq(from=5,to=0.1,length.out=150),0.01,0.0001) #Create a set of tuning parameters, adding low value to also see least squares fit
cv.out=cv.glmnet(x,y,alpha=1,nfolds=10,lambda=lambda, standardize=TRUE) #alpha=1 gives lasso, alpha=0 gives ridge
```


The $\lambda$-value that gives lowest MSE is
```{r, echo=FALSE}

cv.out$lambda.min
```

And the value that corresponds to 1se is
```{r, echo=FALSE}
L = cv.out$lambda.1se
cv.out$lambda.1se

```

We chose the last one to be the optimal $\lambda$, and use this for the next exercise.

Q17:

Fitting a model with lasso using $\lambda = 0.01$ gives the following coefficients

```{r, echo=FALSE}
lasso = glmnet(x, y, alpha=1, lambda = L)

coef(lasso)
```

Thus the model is

$$Y = -21.5 -3.38x_{cylinders} + 0.031x_{displace} - 0.035x_{horsepower}-0.0061x_{weight}+0.12x_{acceleration} +0.78x_{year}+1.68x_{origin2}+2.83x_{origin3}$$

Q18:

The predicted mpg for the car with displace=150, horsepower=100, weight=3000, acceleration=10, year=82 and comes from Europe is
```{r, echo=FALSE}
x= 

P = predict(lasso, newx=matrix(c(0,150,100,3000,10,82,1,0),nrow=1), s=L)
P

```
