---
subtitle: "TMA4268 Statistical Learning V2018"
title: "Compulsory exercise 3: Group XYZ"
author: "NN1, NN2 and NN3"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  # html_document
  pdf_document
---
  
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,results="hold",message = FALSE,warning=FALSE)
```


# Problem 2 - Nonlinear class boundaries and support vector machine

## a) Bayes decision boundary 

* Q11. What is a Bayes classifier, Bayes decision boundary and Bayes error rate?

The Bayes classifier is a classifier that is based on the conditional probability 

$$
Pr(Y=j|X=x_{0}),
$$
the probability that the response is class $j$, given the predictor. The classifier assigns observation $x_0$ to the class $j$ with the largest such probability. For instance, in a two-class situation, the classifier will assign the observation $x_0$ to the class with $Pr(Y=j|X=x_0)>0.5$.

Bayes decision boundary is the class boundaries where the Bayes classifier's prediction is determined. This boundary represents the points where it is the same conditional probability for each class. Continuing with the two-class problem, the Bayes decision boundary will be the set of points in $\mathbb{R}^2$ where the conditional probability defined above is exactly $50\%$ for each class.

The Bayes error rate is the test error rate $$\text{Ave}(I(y_0 \neq \hat{y}_0))$$ one obtains when using Bayes classifier on a test set. In the formula, one averages over the test observations and get the fraction of misclassified observations. It can be shown that the Bayes classifier is the classifier that minimizes the test error rate, but it requires knowledge about the conditional probability distribution of $Y$ given the observations, which one does not have for real datasets. The Bayes error rate can be written as $1-E(\text{maxPr}(Y=j|X))$, where the expectation is over $X$. It is common to regard the Bayes error rate as the classification answer to the irreducible error from the regression setting.

* Q12. When the Bayes decision boundary is known, do we then need a test set?

When the Bayes decision boundary is known, one has in practice the best possible classifier, so effectively, all other classifiers are approximations of the Bayes classifier. As mentioned above, the Bayes error rate can be compared to the irreducible error, and gives a measure on the inherent variance of the data and origins from factors we have not measured. Usually, the purpose of a test set is to assess the performance of a final model, but in this case we already know we have the best possible model. Also, the fact that Bayes decision boundary is known implies that one know the conditional distribution of the response given the observations in the training set. It is then possible to calculate the error rate analytically (which is done for an example in the lectures) and there is no need for a test set.

## b) Support vector machine

* Q13. What is the difference between a support vector classifier and a support vector machine?

The support vector classifier is a relatively simple classifier used when a separating hyperplane does not necessarily exist. It has a linear decision boundary. The difference from a support vector machine (SVM) is that the SVM can be viewed as an expansion of the support vector classifier. By enlargening the feature space using kernels, it allows for non-linear boundaries. By defining this kernel as linear, one has in practice the support vector classifier, as it turns out that this classifier corresponds to the inner product between the observations corresponding to support vectors and the observations in the training set. 

* Q14. What are parameters for the support vector classifier and the support vector machine? How are these chosen above?

 Mathematically, the support vector classifier is the result of the optimization problem

$$
\max_{\beta_0,\beta_1,...,\epsilon_1,...,\epsilon_n,M} M \quad s.t.\quad \sum_{i=1}^{p}\beta_i^2=1, \quad y_i(\beta_0+\beta_1x_{i1}+...+\beta_px_{ip})=y_if(\mathbf{x_i})\geq M(1-\epsilon_i), \quad \epsilon_i \geq 0,\quad \sum_{i=1}^{n}\epsilon_i\leq C.
$$
Here, M is the width of the margin. The resulting hyperplane depend only on the observations that violate the margin and should have normalized coefficients $\beta_i$'s. The variables $\epsilon_i$ are called slack variables and gives information about where each observation are relative to the separating hyperplane and the margin. If $\epsilon_i=0$, then the observation is on the correct side of the margin, if $\epsilon_i>0$, the observation is on the wrong side of the margin and if $\epsilon>1$, the observation is on wrong side of the hyperplane and is thus misclassified. The parameter $C$ is a tuning parameter. It governs the tolerance of violations to the margin. As $C$ increases, more violations are allowed and the margin will increase as well, and the variance will decrease. $C$ is usually chosen via cross validation. In the 'svm()' function in the above code, the variable $C$ is not used, but instead we define a cost function that can be interpreted in a slightly different way; it allows us to specify the influence each violation to the margin has. When this is small, the margin will be wider than if this cost is large. In the above code, the tune()-function performs ten-fold cross-validation on a range of different cost values and it returns the best result in the output, which in turn is used by the svm()-function to fit the best model

By exchanging the above function $f(\mathbf{x_i})$ with a kernel representation $g(\mathbf{x_i})$ where 

$$
g(\mathbf{x})=\beta_0 + \sum_{i \in \mathcal{S}} \alpha_iK(\mathbf{x},\mathbf{x}_i),
$$
we can write the optimization problem as 

$$
\max_{\beta_0,\beta_1,...,\epsilon_1,...,\epsilon_n,M} M \quad s.t.\quad y_ig(\mathbf{x_i})\geq M(1-\epsilon_i), \quad i \in [1,n], \quad \epsilon_i \geq 0,\quad \sum_{i=1}^{n}\epsilon_i\leq C.
$$
In the code, the kernel used is the radial kernel, defined as 

$$
K(x_i,x_i')=\text{exp}\{-\gamma\sum_{j=1}^p (x_{ij}-x_{i'j})^2\},
$$
where the constant $\gamma$ is tuning parameter also chosen by the tune()-function using ten-fold cross-validation. 

* Q15. How would you evaluate the support vector machine decision boundary compared to the Bayes decision boundary?

The SVM decision boundary seems to follow the same trends as the Bayes decision boundary in areas where there are many observations. In areas where there are fewer observations, the two boundaries seems to differ more, which is natural consequence of fewer data points and less information for the model to rely on in these areas. The SVM boundary seems to be somewhat overfit - if new data were used to test the model, there could be some errors in the areas with less data points. This could be prevented using a smaller cost function (allowing for more slack), at the cost of a higher training error rate.






