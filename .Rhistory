#glmroc=roc(response=test$y,predictor=predglm)
#plot(glmroc)
#auc(glmroc)
#Logistic
LOGroc=roc(response=test$y,predictor=probs_wine1)
#K=3
KNN3 = knn(train = train[,-1], test = test[,-1], k = 3, cl = train$y, prob = F)
KNN3probwinning = attributes(knn(train = train[,-1], test = test[,-1], k = 3, cl = train$y, prob = TRUE))$prob
KNN3prob <- ifelse(KNN3 == "0", 1-KNN3probwinning, KNN3probwinning)
KNN3roc=roc(response=test$y,predictor=KNN3prob)
#LDA
#LDAroc=roc(response=test$y,predictor=lda.train_pred$class)
plot(LOGroc,col="green")
lines(KNN3roc,col="red")
#lines(LDAroc,col="blue")
auc(KNN3roc)
auc(KNN9roc)
# ltrain=lda(y~x1+x2,data=train)
# lpred=predict(object = ltrain, newdata = test)$posterior[,1]
# lroc=roc(response=test$y,lpred)
# plot(lroc)
# auc(lroc)
library(pROC)
#glmroc=roc(response=test$y,predictor=predglm)
#plot(glmroc)
#auc(glmroc)
#Logistic
LOGroc=roc(response=test$y,predictor=probs_wine1)
#K=3
KNN3 = knn(train = train[,-1], test = test[,-1], k = 3, cl = train$y, prob = F)
KNN3probwinning = attributes(knn(train = train[,-1], test = test[,-1], k = 3, cl = train$y, prob = TRUE))$prob
KNN3prob <- ifelse(KNN3 == "0", 1-KNN3probwinning, KNN3probwinning)
KNN3roc=roc(response=test$y,predictor=KNN3prob)
#LDA
LDAroc=roc(response=test$y,predictor=lda.train_pred$class)
library(pROC)
#glmroc=roc(response=test$y,predictor=predglm)
#plot(glmroc)
#auc(glmroc)
#Logistic
LOGroc=roc(response=test$y,predictor=probs_wine1)
#K=3
KNN3 = knn(train = train[,-1], test = test[,-1], k = 3, cl = train$y, prob = F)
KNN3probwinning = attributes(knn(train = train[,-1], test = test[,-1], k = 3, cl = train$y, prob = TRUE))$prob
KNN3prob <- ifelse(KNN3 == "0", 1-KNN3probwinning, KNN3probwinning)
KNN3roc=roc(response=test$y,predictor=KNN3prob)
#LDA
LDAroc=roc(response=test$y,predictor=lda.train_pred$posterior)
View(lda.train_pred)
View(lda.train_pred)
library(pROC)
#glmroc=roc(response=test$y,predictor=predglm)
#plot(glmroc)
#auc(glmroc)
#Logistic
LOGroc=roc(response=test$y,predictor=probs_wine1)
#K=3
KNN3 = knn(train = train[,-1], test = test[,-1], k = 3, cl = train$y, prob = F)
KNN3probwinning = attributes(knn(train = train[,-1], test = test[,-1], k = 3, cl = train$y, prob = TRUE))$prob
KNN3prob <- ifelse(KNN3 == "0", 1-KNN3probwinning, KNN3probwinning)
KNN3roc=roc(response=test$y,predictor=KNN3prob)
#LDA
LDAroc=roc(response=test$y,predictor=lda.train_pred$prob)
lda.train_pred[["posterior"]]
library(pROC)
#glmroc=roc(response=test$y,predictor=predglm)
#plot(glmroc)
#auc(glmroc)
#Logistic
LOGroc=roc(response=test$y,predictor=probs_wine1)
#K=3
KNN3 = knn(train = train[,-1], test = test[,-1], k = 3, cl = train$y, prob = F)
KNN3probwinning = attributes(knn(train = train[,-1], test = test[,-1], k = 3, cl = train$y, prob = TRUE))$prob
KNN3prob <- ifelse(KNN3 == "0", 1-KNN3probwinning, KNN3probwinning)
KNN3roc=roc(response=test$y,predictor=KNN3prob)
#LDA
LDAroc=roc(response=test$y,predictor=lda.train_pred$postrior[,2])
library(pROC)
#glmroc=roc(response=test$y,predictor=predglm)
#plot(glmroc)
#auc(glmroc)
#Logistic
LOGroc=roc(response=test$y,predictor=probs_wine1)
#K=3
KNN3 = knn(train = train[,-1], test = test[,-1], k = 3, cl = train$y, prob = F)
KNN3probwinning = attributes(knn(train = train[,-1], test = test[,-1], k = 3, cl = train$y, prob = TRUE))$prob
KNN3prob <- ifelse(KNN3 == "0", 1-KNN3probwinning, KNN3probwinning)
KNN3roc=roc(response=test$y,predictor=KNN3prob)
#LDA
LDAroc=roc(response=test$y,predictor=lda.train_pred$posterior[,2])
plot(LOGroc,col="green")
lines(KNN3roc,col="red")
lines(LDAroc,col="blue")
auc(KNN3roc)
auc(KNN9roc)
# ltrain=lda(y~x1+x2,data=train)
# lpred=predict(object = ltrain, newdata = test)$posterior[,1]
# lroc=roc(response=test$y,lpred)
# plot(lroc)
# auc(lroc)
auc(LOGroc)
auc(KNN3roc)
auc(LDAroc)
auc(LOGroc)
auc(KNN3roc)
auc(LDAroc)
library(ggplot2)
library(GGally)
library(class)
library(MASS)
library(pROC)
wine=read.csv("https://www.math.ntnu.no/emner/TMA4268/2018v/data/Comp1Wine.csv",sep=" ")
wine$class=as.factor(wine$class-1)
colnames(wine)=c("y","x1","x2")
#ggpairs(wine, ggplot2::aes(color=y)) #plotte denne?
n=dim(wine)[1]
set.seed(4029) #to get the same order if you rerun - but you change this to your favorite number
ord = sample(1:n) #shuffle
test = wine[ord[1:(n/2)],]
train = wine[ord[((n/2)+1):n],]
glm.fits=glm(y~x1+x2, family = binomial,data=train)
glm.fits
summary(glm.fits)
b0=glm.fits$coefficients[1]
b1=glm.fits$coefficients[2]
b2=glm.fits$coefficients[3]
#plotting training data
#plot(train$y)
#a=(log(0.5)-b0)/b2
b=-b1/b2
a=(-b0)/b2
ggplot(train,mapping=aes(x=x1,y=x2,color=y))+ggtitle('Training observations')+geom_point() + geom_abline(slope=b, intercept=a)
#Confusion table
probs_wine1<- predict(glm.fits, test,type="response")
#summary(probs_wine1)
pred_wine1<-rep("0",65)
pred_wine1[probs_wine1>0.5]="1"
t_log=table(pred_wine1,(test$y))
t_log
x_1=17
x_2=3
p_i=(exp(b0+b1*x_1+b2*x_2))/(1+exp(b0+b1*x_1+b2*x_2))
p_i
#compute predicted probabilities
predictions_wine1=data.frame(probs_wine1,pred_wine1,test[,1])
colnames(predictions_wine1)=c("Estimated prob","Predicted class","True class")
sens_log=t_log[2,2]/(t_log[1,2]+t_log[2,2])
spec_log=t_log[1,1]/(t_log[1,1]+t_log[2,1])
sens_log
spec_log
library(class)
knn.train = train
knn.test = test
knn1.wine = knn(train = knn.train[,-1], test = knn.test[,-1], cl = train$y, k=3)
tknn3 = table(knn1.wine, test$y)
tknn3
sens_3 = tknn3[2,2]/(tknn3[1,2]+tknn3[2,2])
sens_3
spec_3 = tknn3[1,1]/(tknn3[1,1]+tknn3[2,1])
spec_3
knn1.wine = knn(train = knn.train[,-1], test = knn.test[,-1], cl = train$y, k=9)
tknn9 = table(knn1.wine, test$y)
tknn9
sens_9 = tknn9[2,2]/(tknn9[1,2]+tknn9[2,2])
sens_9
spec_9 = tknn9[1,1]/(tknn9[1,1]+tknn9[2,1])
spec_9
library(dplyr)
n0=sum(train[,1]==1)
n1=nrow(train)-n0
pi=c(n0/n,n1/n)
x_01=train[train$y==0,"x1"]
x_02=train[train$y==0,"x2"]
x_11=train[train$y==1,"x1"]
x_12=train[train$y==1,"x2"]
mu_0=c(mean(x_01),mean(x_02))
mu_1=c(mean(x_11),mean(x_12))
Sigma_0=var(train[train$y==0,2:3])
Sigma_1=var(train[train$y==1,2:3])
Sigma=(n0-1)*Sigma_0/(n0+n1-2)+(n1-1)*Sigma_1/(n0+n1-2)
pi
mu_0
mu_1
Sigma
right_side=0.5*t(mu_1)%*%solve(Sigma)%*%mu_1-0.5*t(mu_0)%*%solve(Sigma)%*%mu_0-log(pi[2]/pi[1])
left_side=solve(Sigma)%*%(mu_1-mu_0)
const_1=left_side[1]
const_2=left_side[2]
train_intercept=right_side/const_2
train_slope=-const_1/const_2
library(MASS)
library(ggplot2)
#Plotting train and test-data.
train_plot=ggplot(train,aes(x=x1,y=x2,color=y))+geom_point(size=2.5)+geom_abline(slope=train_slope,intercept=train_intercept,color="green3")+geom_point(data=test,shape=5,size=2.5)
train_plot
#Performed LDA
lda.train=lda(y~x1+x2,data=train)
lda.train_pred=predict(lda.train,newdata=test)
#Creating confusion table
library(knitr)
library(kableExtra)
conf_table=table(lda.train_pred$class,test$y)
# row1 = c("", "Predicted -", "Predicted +","Total")
# row2 = c("True -", conf_table[1,1], conf_table[1,2],sum(conf_table[1,]))
# row3 = c("True +", conf_table[2,1], conf_table[2,2],sum(conf_table[2,]))
# row4 = c("Total", sum(conf_table[,1]), sum(conf_table[,2])," ")
# table_wine=kable(rbind(row2, row3,row4),col.names=row1,row.names=FALSE)
# table_wine
total=sum(conf_table)
ratio_LDA=(conf_table[1,1]+conf_table[2,2])/total
test_error_LDA=(conf_table[1,2]+conf_table[2,1])/total
#Calculating sensitivity and specificity
sens_LDA=conf_table[2,2]/sum(conf_table[2,])
spec_LDA=conf_table[1,1]/sum(conf_table[1,])
print(sens_LDA)
print(spec_LDA)
print(ratio)
print(test_error)
Sigma_0
Sigma_1
Sigma
sens_log
sens_3
sens_LDA
spec_log
spec_3
spec_LDA
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(warnings=FALSE)
knitr::opts_chunk$set(message =FALSE)
library(pROC)
#glmroc=roc(response=test$y,predictor=predglm)
#plot(glmroc)
#auc(glmroc)
#Logistic
LOGroc=roc(response=test$y,predictor=probs_wine1)
#K=3
KNN3 = knn(train = train[,-1], test = test[,-1], k = 3, cl = train$y, prob = F)
KNN3probwinning = attributes(knn(train = train[,-1], test = test[,-1], k = 3, cl = train$y, prob = TRUE))$prob
KNN3prob <- ifelse(KNN3 == "0", 1-KNN3probwinning, KNN3probwinning)
KNN3roc=roc(response=test$y,predictor=KNN3prob)
#LDA
LDAroc=roc(response=test$y,predictor=lda.train_pred$posterior[,2])
plot(LOGroc,col="green")
lines(KNN3roc,col="red")
lines(LDAroc,col="blue")
auc(LOGroc)
auc(KNN3roc)
auc(LDAroc)
plot(LOGroc,col="green")
lines(KNN3roc,col="red")
lines(LDAroc,col="blue")
points(sens_3,spec_3)
point(sens_LDA,spec_LDA)
plot(LOGroc,col="green")
lines(KNN3roc,col="red")
lines(LDAroc,col="blue")
points(spec_3,sens_3)
plot(LOGroc,col="green")
lines(KNN3roc,col="red")
lines(LDAroc,col="blue")
points(spec_3,sens_3)
points(spec_LDA,sens_LDA)
points(spec_LOG,sens_LOG)
plot(LOGroc,col="green")
lines(KNN3roc,col="red")
lines(LDAroc,col="blue")
points(spec_3,sens_3)
points(spec_LDA,sens_LDA)
points(spec_log,sens_log)
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(warnings=FALSE)
knitr::opts_chunk$set(message =FALSE)
library(pROC)
#glmroc=roc(response=test$y,predictor=predglm)
#plot(glmroc)
#auc(glmroc)
#Logistic
LOGroc=roc(response=test$y,predictor=probs_wine1)
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(warnings=FALSE)
knitr::opts_chunk$set(message =FALSE)
library(ggplot2)
data = read.table("https://www.math.ntnu.no/emner/TMA4268/2018v/data/SYSBPreg3uid.txt")
modelA=lm(-1/sqrt(SYSBP) ~ .,data = data)
# residuls vs fitted
ggplot(modelA, aes(.fitted, .resid)) + geom_point(pch = 21) +
geom_hline(yintercept = 0, linetype = "dashed") +
geom_smooth(se = FALSE, col = "red", size = 0.5, method = "loess") +
labs(x = "Fitted values", y = "Residuals", title = "Fitted values vs. residuals", subtitle =deparse(modelA$call))
# qq-plot of residuals
ggplot(modelA, aes(sample = .stdresid)) +
stat_qq(pch = 19) +
geom_abline(intercept = 0, slope = 1, linetype = "dotted") +
labs(x = "Theoretical quantiles", y = "Standardized residuals", title = "Normal Q-Q", subtitle = deparse(modelA$call))
# normality test
library(nortest)
ad.test(rstudent(modelA))
library(ggplot2)
data = read.table("https://www.math.ntnu.no/emner/TMA4268/2018v/data/SYSBPreg3uid.txt")
modelB=lm(SYSBP ~ .,data = data)
# residuls vs fitted
ggplot(modelB, aes(.fitted, .resid)) + geom_point(pch = 21) +
geom_hline(yintercept = 0, linetype = "dashed") +
geom_smooth(se = FALSE, col = "red", size = 0.5, method = "loess") +
labs(x = "Fitted values", y = "Residuals", title = "Fitted values vs. residuals", subtitle = deparse(modelB$call))
# qq-plot of residuals
ggplot(modelB, aes(sample = .stdresid)) +
stat_qq(pch = 19) +
geom_abline(intercept = 0, slope = 1, linetype = "dotted") +
labs(x = "Theoretical quantiles", y = "Standardized residuals", title = "Normal Q-Q", subtitle = deparse(modelB$call))
# normality test
library(nortest)
ad.test(rstudent(modelB))
data = read.table("https://www.math.ntnu.no/emner/TMA4268/2018v/data/SYSBPreg3uid.txt")
modelA=lm(-1/sqrt(SYSBP) ~ .,data = data)
summary(modelA)
int=confint(modelA,level = 0.99)
int[5,]
data = read.table("https://www.math.ntnu.no/emner/TMA4268/2018v/data/SYSBPreg3uid.txt")
modelA=lm(-1/sqrt(SYSBP) ~ .,data = data)
new=data.frame(SEX=1,AGE=56,CURSMOKE=1,BMI=89/1.75^2,TOTCHOL=200,BPMEDS=0)
new=cbind(1,new)
new
Y=coef(modelA)%*%t(new)
Y
bp=(1/(-Y)^2)
bp
data = read.table("https://www.math.ntnu.no/emner/TMA4268/2018v/data/SYSBPreg3uid.txt")
modelA=lm(-1/sqrt(SYSBP) ~ .,data = data)
new=data.frame(SEX=1,AGE=56,CURSMOKE=1,BMI=89/1.75^2,TOTCHOL=200,BPMEDS=0)
new=cbind(1,new)
predict(modelA,new,interval='predict',level=0.90)
library(ggplot2)
library(GGally)
library(class)
library(MASS)
library(pROC)
wine=read.csv("https://www.math.ntnu.no/emner/TMA4268/2018v/data/Comp1Wine.csv",sep=" ")
wine$class=as.factor(wine$class-1)
colnames(wine)=c("y","x1","x2")
#ggpairs(wine, ggplot2::aes(color=y)) #plotte denne?
n=dim(wine)[1]
set.seed(4268) #to get the same order if you rerun - but you change this to your favorite number
ord = sample(1:n) #shuffle
test = wine[ord[1:(n/2)],]
train = wine[ord[((n/2)+1):n],]
glm.fits=glm(y~x1+x2, family = binomial,data=train)
summary(glm.fits)
b0=glm.fits$coefficients[1]
b1=glm.fits$coefficients[2]
b2=glm.fits$coefficients[3]
#plotting training data
#plot(train$y)
#a=(log(0.5)-b0)/b2
b=-b1/b2
a=(-b0)/b2
ggplot(train,mapping=aes(x=x1,y=x2,color=y))+ggtitle('Training observations')+geom_point() + geom_abline(slope=b, intercept=a)
#Confusion table
probs_wine1<- predict(glm.fits, test,type="response")
#summary(probs_wine1)
pred_wine1<-rep("0",65)
pred_wine1[probs_wine1>0.5]="1"
t_log=table(pred_wine1,(test$y))
t_log
x_1=17
x_2=3
p_i=(exp(b0+b1*x_1+b2*x_2))/(1+exp(b0+b1*x_1+b2*x_2))
p_i
#compute predicted probabilities
predictions_wine1=data.frame(probs_wine1,pred_wine1,test[,1])
colnames(predictions_wine1)=c("Estimated prob","Predicted class","True class")
sens_log=t_log[2,2]/(t_log[1,2]+t_log[2,2])
spec_log=t_log[1,1]/(t_log[1,1]+t_log[2,1])
sens_log
spec_log
library(class)
knn.train = train
knn.test = test
knn1.wine = knn(train = knn.train[,-1], test = knn.test[,-1], cl = train$y, k=3)
tknn3 = table(knn1.wine, test$y)
tknn3
sens_3 = tknn3[2,2]/(tknn3[1,2]+tknn3[2,2])
sens_3
spec_3 = tknn3[1,1]/(tknn3[1,1]+tknn3[2,1])
spec_3
knn1.wine = knn(train = knn.train[,-1], test = knn.test[,-1], cl = train$y, k=9)
tknn9 = table(knn1.wine, test$y)
tknn9
sens_9 = tknn9[2,2]/(tknn9[1,2]+tknn9[2,2])
sens_9
spec_9 = tknn9[1,1]/(tknn9[1,1]+tknn9[2,1])
spec_9
library(dplyr)
n0=sum(train[,1]==1)
n1=nrow(train)-n0
pi=c(n0/n,n1/n)
x_01=train[train$y==0,"x1"]
x_02=train[train$y==0,"x2"]
x_11=train[train$y==1,"x1"]
x_12=train[train$y==1,"x2"]
mu_0=c(mean(x_01),mean(x_02))
mu_1=c(mean(x_11),mean(x_12))
Sigma_0=var(train[train$y==0,2:3])
Sigma_1=var(train[train$y==1,2:3])
Sigma=(n0-1)*Sigma_0/(n0+n1-2)+(n1-1)*Sigma_1/(n0+n1-2)
pi
mu_0
mu_1
Sigma
right_side=0.5*t(mu_1)%*%solve(Sigma)%*%mu_1-0.5*t(mu_0)%*%solve(Sigma)%*%mu_0-log(pi[2]/pi[1])
left_side=solve(Sigma)%*%(mu_1-mu_0)
const_1=left_side[1]
const_2=left_side[2]
train_intercept=right_side/const_2
train_slope=-const_1/const_2
library(MASS)
library(ggplot2)
#Plotting train and test-data.
train_plot=ggplot(train,aes(x=x1,y=x2,color=y))+geom_point(size=2.5)+geom_abline(slope=train_slope,intercept=train_intercept,color="green3")+geom_point(data=test,shape=5,size=2.5)
train_plot
#Performed LDA
lda.train=lda(y~x1+x2,data=train)
lda.train_pred=predict(lda.train,newdata=test)
#Creating confusion table
library(knitr)
library(kableExtra)
conf_table=table(lda.train_pred$class,test$y)
# row1 = c("", "Predicted -", "Predicted +","Total")
# row2 = c("True -", conf_table[1,1], conf_table[1,2],sum(conf_table[1,]))
# row3 = c("True +", conf_table[2,1], conf_table[2,2],sum(conf_table[2,]))
# row4 = c("Total", sum(conf_table[,1]), sum(conf_table[,2])," ")
# table_wine=kable(rbind(row2, row3,row4),col.names=row1,row.names=FALSE)
# table_wine
total=sum(conf_table)
ratio_LDA=(conf_table[1,1]+conf_table[2,2])/total
test_error_LDA=(conf_table[1,2]+conf_table[2,1])/total
#Calculating sensitivity and specificity
sens_LDA=conf_table[2,2]/sum(conf_table[2,])
spec_LDA=conf_table[1,1]/sum(conf_table[1,])
print(sens_LDA)
print(spec_LDA)
print(ratio_LDA)
print(test_error_LDA)
Sigma_0
Sigma_1
Sigma
sens_log
sens_3
sens_LDA
spec_log
spec_3
spec_LDA
library(pROC)
#glmroc=roc(response=test$y,predictor=predglm)
#plot(glmroc)
#auc(glmroc)
#Logistic
LOGroc=roc(response=test$y,predictor=probs_wine1)
#K=3
KNN3 = knn(train = train[,-1], test = test[,-1], k = 3, cl = train$y, prob = F)
KNN3probwinning = attributes(knn(train = train[,-1], test = test[,-1], k = 3, cl = train$y, prob = TRUE))$prob
KNN3prob <- ifelse(KNN3 == "0", 1-KNN3probwinning, KNN3probwinning)
KNN3roc=roc(response=test$y,predictor=KNN3prob)
#LDA
LDAroc=roc(response=test$y,predictor=lda.train_pred$posterior[,2])
plot(LOGroc,col="green")
lines(KNN3roc,col="red")
lines(LDAroc,col="blue")
auc(LOGroc)
auc(KNN3roc)
auc(LDAroc)
View(predictions_wine1)
right_side=0.5*t(mu_1)%*%solve(Sigma)%*%mu_1-0.5*t(mu_0)%*%solve(Sigma)%*%mu_0-log(pi[2]/pi[1])
left_side=solve(Sigma)%*%(mu_1-mu_0)
const_1=left_side[1]
const_2=left_side[2]
train_intercept=right_side/const_2
train_slope=-const_1/const_2
right_side<-0.5*t(mu_1)%*%solve(Sigma)%*%mu_1-0.5*t(mu_0)%*%solve(Sigma)%*%mu_0-log(pi[2]/pi[1])
left_side=solve(Sigma)%*%(mu_1-mu_0)
const_1=left_side[1]
const_2=left_side[2]
train_intercept=right_side/const_2
train_slope=-const_1/const_2
right_side
right_side<-0.5*t(mu_1)%*%solve(Sigma)%*%mu_1-0.5*t(mu_0)%*%solve(Sigma)%*%mu_0-log(pi[2]/pi[1])
left_side=solve(Sigma)%*%(mu_1-mu_0)
const_1=left_side[1]
const_2=left_side[2]
train_intercept=right_side/const_2
train_slope=-const_1/const_2
right_side
train_intercept
train_slope
right_side<-0.5*t(mu_1)%*%solve(Sigma)%*%mu_1-0.5*t(mu_0)%*%solve(Sigma)%*%mu_0-log(pi[2]/pi[1])
left_side=solve(Sigma)%*%(mu_1-mu_0)
const_1=left_side[1]
const_2=left_side[2]
train_intercept=right_side/const_2
train_slope=-const_1/const_2
right_side
train_intercept
train_slope
const_1
const_2
plot(LOGroc,col="green")
lines(KNN3roc,col="red")
lines(LDAroc,col="blue")
auc(LOGroc)
auc(KNN3roc)
auc(LDAroc)
