* $logit(p_i)=log(\frac{p_i}{1-p_i})=log(p_i)-log(1-p_i)=log(\frac{e^{\beta_0 + \beta_1x_{i1} + \beta_2 x_{i2}}}{ 1+ e^{\beta_0 + \beta_1x_{i1} + \beta_2x_{i2}}})-log(1-\frac{e^{\beta_0 + \beta_1x_{i1} + \beta_2 x_{i2}}}{ 1+ e^{\beta_0 + \beta_1x_{i1} + \beta_2x_{i2}}})=$
glm.fits<-glm(y~x1+x2, family = binomial,data=train)
summary(glm.fits)
?glm()
b0=glm.fits$coefficients[1]
b1=glm.fits$coefficients[2]
b2=glm.fits$coefficients[3]
library(ggplot2)
library(GGally)
library(class)
library(MASS)
library(pROC)
wine=read.csv("https://www.math.ntnu.no/emner/TMA4268/2018v/data/Comp1Wine.csv",sep=" ")
wine$class=as.factor(wine$class-1)
colnames(wine)=c("y","x1","x2")
ggpairs(wine, ggplot2::aes(color=y))
n=dim(wine)[1]
set.seed(1900) #to get the same order if you rerun - but you change this to your favorite number
ord = sample(1:n) #shuffle
test = wine[ord[1:(n/2)],]
train = wine[ord[((n/2)+1):n],]
head(train)
glm.fits<-glm(y~x1+x2, family = binomial,data=train)
b0=glm.fits$coefficients[1]
b1=glm.fits$coefficients[2]
b2=glm.fits$coefficients[3]
plot(train$y)
b=-b1/b2
a=(-b0)/b2
p<-ggplot(train,mapping=aes(x=x1,y=x2,color=y))+ggtitle('Training observations')+geom_point() +geom_abline(slope=b, intercept=a)
p
summary(glm.fits)
p<-ggplot(train,mapping=aes(x=x1,y=x2,color=y))+ggtitle('Training observations')+geom_point() +geom_abline(slope=b, intercept=a) + stat_function(fun=function(x) exp(b0+x*b1+x*b2)/(1+exp(b0+x*b1 +x*b2)), geom="line", colour="red",linetype="dashed")
p
ggplot(stat_function(fun=function(x) exp(b0+x*b1+x*b2)/(1+exp(b0+x*b1 +x*b2)), geom="line", colour="red",linetype="dashed"))
p
p<-ggplot(train,mapping=aes(x=x1,y=x2,color=y))+ggtitle('Training observations')+geom_point() +
geom_abline(slope=b, intercept=a)
p
plot(stat_function(fun=function(x) exp(b0+x*b1+x*b2)/(1+exp(b0+x*b1 +x*b2)), geom="line", colour="red",linetype="dashed"))
stat_function(fun=function(x) exp(b0+x*b1+x*b2)/(1+exp(b0+x*b1 +x*b2)), geom="line", colour="red",linetype="dashed")
plott=stat_function(fun=function(x) exp(b0+x*b1+x*b2)/(1+exp(b0+x*b1 +x*b2)), geom="line", colour="red",linetype="dashed")
p
plott=stat_function(fun=function(x) exp(b0+x*b1+x*b2)/(1+exp(b0+x*b1 +x*b2)), geom="line", colour="red",linetype="dashed")
ggplot(data.frame(x=c(-6,5)), aes(x))+
xlab(expression(x))+
ylab(expression(mu))+ plott)
ggplot(data.frame(x=c(-6,5)), aes(x))+
xlab(expression(x))+
ylab(expression(mu))+ plott)
ggplot(data.frame(x=c(-6,5)), aes(x))+
xlab(expression(x))+
ylab(expression(mu))+ plott
b0=glm.fits$coefficients[1]
b1=glm.fits$coefficients[2]
b2=glm.fits$coefficients[3]
summary(glm.fits)
coef(glm.fits)
library(ggplot2)
library(GGally)
library(class)
library(MASS)
library(pROC)
wine=read.csv("https://www.math.ntnu.no/emner/TMA4268/2018v/data/Comp1Wine.csv",sep=" ")
wine$class=as.factor(wine$class-1)
colnames(wine)=c("y","x1","x2")
ggpairs(wine, ggplot2::aes(color=y))
n=dim(wine)[1]
set.seed(1900) #to get the same order if you rerun - but you change this to your favorite number
ord = sample(1:n) #shuffle
test = wine[ord[1:(n/2)],]
train = wine[ord[((n/2)+1):n],]
knitr::opts_chunk$set(echo = TRUE)
p<-ggplot(train,mapping=aes(x=x1,y=x2,color=y))+ggtitle('Training observations')+geom_point() +
geom_abline(slope=b, intercept=a)
p
p
glm.fits<-glm(y~x1+x2, family = binomial,data=train)
summary(glm.fits)
p<-ggplot(train,mapping=aes(x=x1,y=x2,color=y))+ggtitle('Training observations')+geom_point() +
geom_abline(slope=b, intercept=a)
p
```{r setup, echo=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(GGally)
library(class)
library(MASS)
library(pROC)
wine=read.csv("https://www.math.ntnu.no/emner/TMA4268/2018v/data/Comp1Wine.csv",sep=" ")
wine$class=as.factor(wine$class-1)
colnames(wine)=c("y","x1","x2")
n=dim(wine)[1]
set.seed(1900) #to get the same order if you rerun - but you change this to your favorite number
ord = sample(1:n) #shuffle
test = wine[ord[1:(n/2)],]
train = wine[ord[((n/2)+1):n],]
glm.fits<-glm(y~x1+x2, family = binomial,data=train)
#summary(glm.fits)
#coef(glm.fits)
```{r, echo=FALSE}
```{r, echo=FALSE}
