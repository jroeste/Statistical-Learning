x_01=train[train$y==0,"x1"]
x_02=train[train$y==0,"x2"]
x_11=train[train$y==1,"x1"]
x_12=train[train$y==1,"x2"]
mu_0=c(mean(x_01),mean(x_02))
mu_1=c(mean(x_11),mean(x_12))
Sigma_0=var(train[train$y==0,2:3])
Sigma_1=var(train[train$y==1,2:3])
Sigma=(n0-1)*Sigma_0/(n0+n1-2)+(n1-1)*Sigma_1/(n0+n1-2)
Sigma
mu_0
mu_1
pi
right_side=0.5*t(mu_1)%*%solve(Sigma)%*%mu_1-0.5*t(mu_0)%*%solve(Sigma)%*%mu_0-log(pi[2]/pi[1])
left_side=solve(Sigma)%*%(mu_1-mu_0)
const_1=left_side[1]
const_2=left_side[2]
train_intercept=right_side/const_2
train_slope=-const_1/const_2
library(MASS)
library(ggplot2)
#Plotting train and test-data.
train_plot=ggplot(train,aes(x=x1,y=x2,color=y))+geom_point(size=2.5)+geom_abline(slope=train_slope,intercept=train_intercept,color="green3")+geom_point(data=test,shape=5,size=2.5)
train_plot
#Performed LDA
lda.train=lda(y~x1+x2,data=train)
lda.train_pred=predict(lda.train,newdata=test)
lda.train_pred
conf_table=table(lda.train_pred$class,test$y)
#Creating confusion table
library(knitr)
library(kableExtra)
row1 = c("", "Predicted -", "Predicted +","Total")
row2 = c("True -", conf_table[1,1], conf_table[1,2],sum(conf_table[1,]))
row3 = c("True +", conf_table[2,1], conf_table[2,2],sum(conf_table[2,]))
row4 = c("Total", sum(conf_table[,1]), sum(conf_table[,2])," ")
table_wine=kable(rbind(row2, row3,row4),col.names=row1,row.names=FALSE)
table_wine
total=sum(conf_table)
ratio=(conf_table[1,1]+conf_table[2,2])/total
test_error=(conf_table[1,2]+conf_table[2,1])/total
#Calculating sensitivity and specificity
wine_sens=conf_table[2,2]/sum(conf_table[2,])
wine_spec=conf_table[1,1]/sum(conf_table[1,])
print(wine_sens)
print(wine_spec)
print(ratio)
print(test_error)
library(pROC)
#glmroc=roc(response=test$y,predictor=predglm)
#plot(glmroc)
#auc(glmroc)
#K=3
KNN3 = knn(train = train[,-1], test = test[,-1], k = 3, cl = train$y, prob = F)
KNN3probwinning = attributes(knn(train = train[,-1], test = test[,-1], k = 3, cl = train$y, prob = TRUE))$prob
KNN3prob <- ifelse(KNN3 == "0", 1-KNN3probwinning, KNN3probwinning)
KNN3roc=roc(response=test$y,predictor=KNN3prob)
KNN3roc
plot(KNN3roc)
auc(KNN3roc)
points(spec_3,sens_3)
#K=9
KNN9 = knn(train = train[,-1], test = test[,-1], k = 9, cl = train$y, prob = F)
KNN9probwinning = attributes(knn(train = train[,-1], test = test[,-1], k = 9, cl = train$y, prob = TRUE))$prob
KNN9prob <- ifelse(KNN9 == "0", 1-KNN9probwinning, KNN9probwinning)
KNN9roc=roc(response=test$y,predictor=KNN9prob)
plot(KNN3roc)
lines(KNN9roc)
auc(KNN9roc)
points(spec_9,sens_9)
# ltrain=lda(y~x1+x2,data=train)
# lpred=predict(object = ltrain, newdata = test)$posterior[,1]
# lroc=roc(response=test$y,lpred)
# plot(lroc)
# auc(lroc)
plot(KNN3roc,col="green")
library(pROC)
#glmroc=roc(response=test$y,predictor=predglm)
#plot(glmroc)
#auc(glmroc)
#K=3
KNN3 = knn(train = train[,-1], test = test[,-1], k = 3, cl = train$y, prob = F)
KNN3probwinning = attributes(knn(train = train[,-1], test = test[,-1], k = 3, cl = train$y, prob = TRUE))$prob
KNN3prob <- ifelse(KNN3 == "0", 1-KNN3probwinning, KNN3probwinning)
KNN3roc=roc(response=test$y,predictor=KNN3prob)
KNN3roc
plot(KNN3roc)
auc(KNN3roc)
points(spec_3,sens_3)
#K=9
KNN9 = knn(train = train[,-1], test = test[,-1], k = 9, cl = train$y, prob = F)
KNN9probwinning = attributes(knn(train = train[,-1], test = test[,-1], k = 9, cl = train$y, prob = TRUE))$prob
KNN9prob <- ifelse(KNN9 == "0", 1-KNN9probwinning, KNN9probwinning)
KNN9roc=roc(response=test$y,predictor=KNN9prob)
plot(KNN3roc,col="green")
lines(KNN9roc,col="red")
auc(KNN9roc)
points(spec_9,sens_9)
# ltrain=lda(y~x1+x2,data=train)
# lpred=predict(object = ltrain, newdata = test)$posterior[,1]
# lroc=roc(response=test$y,lpred)
# plot(lroc)
# auc(lroc)
View(Sigma_0)
View(Sigma_0)
View(Sigma_1)
View(Sigma_1)
n=dim(wine)[1]
set.seed(1828) #to get the same order if you rerun - but you change this to your favorite number
ord = sample(1:n) #shuffle
test = wine[ord[1:(n/2)],]
train = wine[ord[((n/2)+1):n],]
glm.fits=glm(y~x1+x2, family = binomial,data=train)
summary(glm.fits)
b0=glm.fits$coefficients[1]
b1=glm.fits$coefficients[2]
b2=glm.fits$coefficients[3]
#plotting training data
#plot(train$y)
#a=(log(0.5)-b0)/b2
b=-b1/b2
a=(-b0)/b2
ggplot(train,mapping=aes(x=x1,y=x2,color=y))+ggtitle('Training observations')+geom_point() + geom_abline(slope=b, intercept=a)
#Confusion table
probs_wine1<- predict(glm.fits, test,type="response")
#summary(probs_wine1)
pred_wine1<-rep("0",65)
pred_wine1[probs_wine1>0.5]="1"
t_log=table(pred_wine1,(test$y))
t_log
x_1=17
x_2=3
p_i=(exp(b0+b1*x_1+b2*x_2))/(1+exp(b0+b1*x_1+b2*x_2))
p_i
#compute predicted probabilities
predictions_wine1=data.frame(probs_wine1,pred_wine1,test[,1])
colnames(predictions_wine1)=c("Estimated prob","Predicted class","True class")
sens_log=t_log[2,2]/(t_log[1,2]+t_log[2,2])
spec_log=t_log[1,1]/(t_log[1,1]+t_log[2,1])
sens_log
spec_log
library(class)
knn.train = train
knn.test = test
knn1.wine = knn(train = knn.train[,-1], test = knn.test[,-1], cl = train$y, k=3)
tknn3 = table(knn1.wine, test$y)
tknn3
sens_3 = tknn3[2,2]/(tknn3[1,2]+tknn3[2,2])
sens_3
spec_3 = tknn3[1,1]/(tknn3[1,1]+tknn3[2,1])
spec_3
knn1.wine = knn(train = knn.train[,-1], test = knn.test[,-1], cl = train$y, k=9)
tknn9 = table(knn1.wine, test$y)
tknn9
sens_9 = tknn9[2,2]/(tknn9[1,2]+tknn9[2,2])
sens_9
spec_9 = tknn9[1,1]/(tknn9[1,1]+tknn9[2,1])
spec_9
library(dplyr)
n0=sum(train[,1]==1)
n1=nrow(train)-n0
pi=c(n0/n,n1/n)
x_01=train[train$y==0,"x1"]
x_02=train[train$y==0,"x2"]
x_11=train[train$y==1,"x1"]
x_12=train[train$y==1,"x2"]
mu_0=c(mean(x_01),mean(x_02))
mu_1=c(mean(x_11),mean(x_12))
Sigma_0=var(train[train$y==0,2:3])
Sigma_1=var(train[train$y==1,2:3])
Sigma=(n0-1)*Sigma_0/(n0+n1-2)+(n1-1)*Sigma_1/(n0+n1-2)
pi
mu_0
mu_1
Sigma
right_side=0.5*t(mu_1)%*%solve(Sigma)%*%mu_1-0.5*t(mu_0)%*%solve(Sigma)%*%mu_0-log(pi[2]/pi[1])
left_side=solve(Sigma)%*%(mu_1-mu_0)
const_1=left_side[1]
const_2=left_side[2]
train_intercept=right_side/const_2
train_slope=-const_1/const_2
library(MASS)
library(ggplot2)
#Plotting train and test-data.
train_plot=ggplot(train,aes(x=x1,y=x2,color=y))+geom_point(size=2.5)+geom_abline(slope=train_slope,intercept=train_intercept,color="green3")+geom_point(data=test,shape=5,size=2.5)
train_plot
#Performed LDA
lda.train=lda(y~x1+x2,data=train)
lda.train_pred=predict(lda.train,newdata=test)
#Creating confusion table
library(knitr)
library(kableExtra)
conf_table=table(lda.train_pred$class,test$y)
# row1 = c("", "Predicted -", "Predicted +","Total")
# row2 = c("True -", conf_table[1,1], conf_table[1,2],sum(conf_table[1,]))
# row3 = c("True +", conf_table[2,1], conf_table[2,2],sum(conf_table[2,]))
# row4 = c("Total", sum(conf_table[,1]), sum(conf_table[,2])," ")
# table_wine=kable(rbind(row2, row3,row4),col.names=row1,row.names=FALSE)
# table_wine
total=sum(conf_table)
ratio_LDA=(conf_table[1,1]+conf_table[2,2])/total
test_error_LDA=(conf_table[1,2]+conf_table[2,1])/total
#Calculating sensitivity and specificity
sens_LDA=conf_table[2,2]/sum(conf_table[2,])
spec_LDA=conf_table[1,1]/sum(conf_table[1,])
print(sens_LDA)
print(spec_LDA)
print(ratio)
print(test_error)
Sigma_0
Sigma_1
Sigma_0
Sigma_1
Sigma
library(pROC)
#glmroc=roc(response=test$y,predictor=predglm)
#plot(glmroc)
#auc(glmroc)
#Logistic
#LOGroc=roc(response=test$y,predictor=)
#K=3
KNN3 = knn(train = train[,-1], test = test[,-1], k = 3, cl = train$y, prob = F)
KNN3probwinning = attributes(knn(train = train[,-1], test = test[,-1], k = 3, cl = train$y, prob = TRUE))$prob
KNN3prob <- ifelse(KNN3 == "0", 1-KNN3probwinning, KNN3probwinning)
KNN3prob
KNN3roc=roc(response=test$y,predictor=KNN3prob)
KNN3roc
plot(KNN3roc,col="green")
lines(KNN9roc,col="red")
auc(KNN3roc)
auc(KNN9roc)
# ltrain=lda(y~x1+x2,data=train)
# lpred=predict(object = ltrain, newdata = test)$posterior[,1]
# lroc=roc(response=test$y,lpred)
# plot(lroc)
# auc(lroc)
n=dim(wine)[1]
set.seed(1828) #to get the same order if you rerun - but you change this to your favorite number
ord = sample(1:n) #shuffle
test = wine[ord[1:(n/2)],]
train = wine[ord[((n/2)+1):n],]
glm.fits=glm(y~x1+x2, family = binomial,data=train)
glm.fits
summary(glm.fits)
library(ggplot2)
library(GGally)
library(class)
library(MASS)
library(pROC)
wine=read.csv("https://www.math.ntnu.no/emner/TMA4268/2018v/data/Comp1Wine.csv",sep=" ")
wine$class=as.factor(wine$class-1)
colnames(wine)=c("y","x1","x2")
#ggpairs(wine, ggplot2::aes(color=y)) #plotte denne?
n=dim(wine)[1]
set.seed(1828) #to get the same order if you rerun - but you change this to your favorite number
ord = sample(1:n) #shuffle
test = wine[ord[1:(n/2)],]
train = wine[ord[((n/2)+1):n],]
glm.fits=glm(y~x1+x2, family = binomial,data=train)
glm.fits
summary(glm.fits)
b0=glm.fits$coefficients[1]
b1=glm.fits$coefficients[2]
b2=glm.fits$coefficients[3]
#plotting training data
#plot(train$y)
#a=(log(0.5)-b0)/b2
b=-b1/b2
a=(-b0)/b2
ggplot(train,mapping=aes(x=x1,y=x2,color=y))+ggtitle('Training observations')+geom_point() + geom_abline(slope=b, intercept=a)
#Confusion table
probs_wine1<- predict(glm.fits, test,type="response")
#summary(probs_wine1)
pred_wine1<-rep("0",65)
pred_wine1[probs_wine1>0.5]="1"
t_log=table(pred_wine1,(test$y))
t_log
x_1=17
x_2=3
p_i=(exp(b0+b1*x_1+b2*x_2))/(1+exp(b0+b1*x_1+b2*x_2))
p_i
#compute predicted probabilities
predictions_wine1=data.frame(probs_wine1,pred_wine1,test[,1])
colnames(predictions_wine1)=c("Estimated prob","Predicted class","True class")
sens_log=t_log[2,2]/(t_log[1,2]+t_log[2,2])
spec_log=t_log[1,1]/(t_log[1,1]+t_log[2,1])
sens_log
spec_log
library(class)
knn.train = train
knn.test = test
knn1.wine = knn(train = knn.train[,-1], test = knn.test[,-1], cl = train$y, k=3)
tknn3 = table(knn1.wine, test$y)
tknn3
sens_3 = tknn3[2,2]/(tknn3[1,2]+tknn3[2,2])
sens_3
spec_3 = tknn3[1,1]/(tknn3[1,1]+tknn3[2,1])
spec_3
knn1.wine = knn(train = knn.train[,-1], test = knn.test[,-1], cl = train$y, k=9)
tknn9 = table(knn1.wine, test$y)
tknn9
sens_9 = tknn9[2,2]/(tknn9[1,2]+tknn9[2,2])
sens_9
spec_9 = tknn9[1,1]/(tknn9[1,1]+tknn9[2,1])
spec_9
library(dplyr)
n0=sum(train[,1]==1)
n1=nrow(train)-n0
pi=c(n0/n,n1/n)
x_01=train[train$y==0,"x1"]
x_02=train[train$y==0,"x2"]
x_11=train[train$y==1,"x1"]
x_12=train[train$y==1,"x2"]
mu_0=c(mean(x_01),mean(x_02))
mu_1=c(mean(x_11),mean(x_12))
Sigma_0=var(train[train$y==0,2:3])
Sigma_1=var(train[train$y==1,2:3])
Sigma=(n0-1)*Sigma_0/(n0+n1-2)+(n1-1)*Sigma_1/(n0+n1-2)
pi
mu_0
mu_1
Sigma
right_side=0.5*t(mu_1)%*%solve(Sigma)%*%mu_1-0.5*t(mu_0)%*%solve(Sigma)%*%mu_0-log(pi[2]/pi[1])
left_side=solve(Sigma)%*%(mu_1-mu_0)
const_1=left_side[1]
const_2=left_side[2]
train_intercept=right_side/const_2
train_slope=-const_1/const_2
library(MASS)
library(ggplot2)
#Plotting train and test-data.
train_plot=ggplot(train,aes(x=x1,y=x2,color=y))+geom_point(size=2.5)+geom_abline(slope=train_slope,intercept=train_intercept,color="green3")+geom_point(data=test,shape=5,size=2.5)
train_plot
#Performed LDA
lda.train=lda(y~x1+x2,data=train)
lda.train_pred=predict(lda.train,newdata=test)
#Creating confusion table
library(knitr)
library(kableExtra)
conf_table=table(lda.train_pred$class,test$y)
# row1 = c("", "Predicted -", "Predicted +","Total")
# row2 = c("True -", conf_table[1,1], conf_table[1,2],sum(conf_table[1,]))
# row3 = c("True +", conf_table[2,1], conf_table[2,2],sum(conf_table[2,]))
# row4 = c("Total", sum(conf_table[,1]), sum(conf_table[,2])," ")
# table_wine=kable(rbind(row2, row3,row4),col.names=row1,row.names=FALSE)
# table_wine
total=sum(conf_table)
ratio_LDA=(conf_table[1,1]+conf_table[2,2])/total
test_error_LDA=(conf_table[1,2]+conf_table[2,1])/total
#Calculating sensitivity and specificity
sens_LDA=conf_table[2,2]/sum(conf_table[2,])
spec_LDA=conf_table[1,1]/sum(conf_table[1,])
print(sens_LDA)
print(spec_LDA)
print(ratio)
print(test_error)
Sigma_0
Sigma_1
Sigma
sens_log
sens_3
sens_LDA
spec_log
spec_3
spec_LDA
library(pROC)
#glmroc=roc(response=test$y,predictor=predglm)
#plot(glmroc)
#auc(glmroc)
#Logistic
LOGroc=roc(response=test$y,predictor=probs_wine1)
#K=3
KNN3 = knn(train = train[,-1], test = test[,-1], k = 3, cl = train$y, prob = F)
KNN3probwinning = attributes(knn(train = train[,-1], test = test[,-1], k = 3, cl = train$y, prob = TRUE))$prob
KNN3prob <- ifelse(KNN3 == "0", 1-KNN3probwinning, KNN3probwinning)
KNN3roc=roc(response=test$y,predictor=KNN3prob)
#LDA
LDAroc=roc(response=test$y,predictor=lda.train_pred)
library(pROC)
#glmroc=roc(response=test$y,predictor=predglm)
#plot(glmroc)
#auc(glmroc)
#Logistic
LOGroc=roc(response=test$y,predictor=probs_wine1)
#K=3
KNN3 = knn(train = train[,-1], test = test[,-1], k = 3, cl = train$y, prob = F)
KNN3probwinning = attributes(knn(train = train[,-1], test = test[,-1], k = 3, cl = train$y, prob = TRUE))$prob
KNN3prob <- ifelse(KNN3 == "0", 1-KNN3probwinning, KNN3probwinning)
KNN3roc=roc(response=test$y,predictor=KNN3prob)
#LDA
LDAroc=roc(response=test$y,predictor=lda.train_pred$class)
library(pROC)
#glmroc=roc(response=test$y,predictor=predglm)
#plot(glmroc)
#auc(glmroc)
#Logistic
LOGroc=roc(response=test$y,predictor=probs_wine1)
#K=3
KNN3 = knn(train = train[,-1], test = test[,-1], k = 3, cl = train$y, prob = F)
KNN3probwinning = attributes(knn(train = train[,-1], test = test[,-1], k = 3, cl = train$y, prob = TRUE))$prob
KNN3prob <- ifelse(KNN3 == "0", 1-KNN3probwinning, KNN3probwinning)
KNN3roc=roc(response=test$y,predictor=KNN3prob)
#LDA
#LDAroc=roc(response=test$y,predictor=lda.train_pred$class)
plot(Logroc,col="green")
library(pROC)
#glmroc=roc(response=test$y,predictor=predglm)
#plot(glmroc)
#auc(glmroc)
#Logistic
LOGroc=roc(response=test$y,predictor=probs_wine1)
#K=3
KNN3 = knn(train = train[,-1], test = test[,-1], k = 3, cl = train$y, prob = F)
KNN3probwinning = attributes(knn(train = train[,-1], test = test[,-1], k = 3, cl = train$y, prob = TRUE))$prob
KNN3prob <- ifelse(KNN3 == "0", 1-KNN3probwinning, KNN3probwinning)
KNN3roc=roc(response=test$y,predictor=KNN3prob)
#LDA
#LDAroc=roc(response=test$y,predictor=lda.train_pred$class)
plot(LOGroc,col="green")
lines(KNN3roc,col="red")
#lines(LDAroc,col="blue")
auc(KNN3roc)
auc(KNN9roc)
# ltrain=lda(y~x1+x2,data=train)
# lpred=predict(object = ltrain, newdata = test)$posterior[,1]
# lroc=roc(response=test$y,lpred)
# plot(lroc)
# auc(lroc)
library(pROC)
#glmroc=roc(response=test$y,predictor=predglm)
#plot(glmroc)
#auc(glmroc)
#Logistic
LOGroc=roc(response=test$y,predictor=probs_wine1)
#K=3
KNN3 = knn(train = train[,-1], test = test[,-1], k = 3, cl = train$y, prob = F)
KNN3probwinning = attributes(knn(train = train[,-1], test = test[,-1], k = 3, cl = train$y, prob = TRUE))$prob
KNN3prob <- ifelse(KNN3 == "0", 1-KNN3probwinning, KNN3probwinning)
KNN3roc=roc(response=test$y,predictor=KNN3prob)
#LDA
LDAroc=roc(response=test$y,predictor=lda.train_pred$class)
library(pROC)
#glmroc=roc(response=test$y,predictor=predglm)
#plot(glmroc)
#auc(glmroc)
#Logistic
LOGroc=roc(response=test$y,predictor=probs_wine1)
#K=3
KNN3 = knn(train = train[,-1], test = test[,-1], k = 3, cl = train$y, prob = F)
KNN3probwinning = attributes(knn(train = train[,-1], test = test[,-1], k = 3, cl = train$y, prob = TRUE))$prob
KNN3prob <- ifelse(KNN3 == "0", 1-KNN3probwinning, KNN3probwinning)
KNN3roc=roc(response=test$y,predictor=KNN3prob)
#LDA
LDAroc=roc(response=test$y,predictor=lda.train_pred$posterior)
View(lda.train_pred)
View(lda.train_pred)
library(pROC)
#glmroc=roc(response=test$y,predictor=predglm)
#plot(glmroc)
#auc(glmroc)
#Logistic
LOGroc=roc(response=test$y,predictor=probs_wine1)
#K=3
KNN3 = knn(train = train[,-1], test = test[,-1], k = 3, cl = train$y, prob = F)
KNN3probwinning = attributes(knn(train = train[,-1], test = test[,-1], k = 3, cl = train$y, prob = TRUE))$prob
KNN3prob <- ifelse(KNN3 == "0", 1-KNN3probwinning, KNN3probwinning)
KNN3roc=roc(response=test$y,predictor=KNN3prob)
#LDA
LDAroc=roc(response=test$y,predictor=lda.train_pred$prob)
lda.train_pred[["posterior"]]
library(pROC)
#glmroc=roc(response=test$y,predictor=predglm)
#plot(glmroc)
#auc(glmroc)
#Logistic
LOGroc=roc(response=test$y,predictor=probs_wine1)
#K=3
KNN3 = knn(train = train[,-1], test = test[,-1], k = 3, cl = train$y, prob = F)
KNN3probwinning = attributes(knn(train = train[,-1], test = test[,-1], k = 3, cl = train$y, prob = TRUE))$prob
KNN3prob <- ifelse(KNN3 == "0", 1-KNN3probwinning, KNN3probwinning)
KNN3roc=roc(response=test$y,predictor=KNN3prob)
#LDA
LDAroc=roc(response=test$y,predictor=lda.train_pred$postrior[,2])
library(pROC)
#glmroc=roc(response=test$y,predictor=predglm)
#plot(glmroc)
#auc(glmroc)
#Logistic
LOGroc=roc(response=test$y,predictor=probs_wine1)
#K=3
KNN3 = knn(train = train[,-1], test = test[,-1], k = 3, cl = train$y, prob = F)
KNN3probwinning = attributes(knn(train = train[,-1], test = test[,-1], k = 3, cl = train$y, prob = TRUE))$prob
KNN3prob <- ifelse(KNN3 == "0", 1-KNN3probwinning, KNN3probwinning)
KNN3roc=roc(response=test$y,predictor=KNN3prob)
#LDA
LDAroc=roc(response=test$y,predictor=lda.train_pred$posterior[,2])
plot(LOGroc,col="green")
lines(KNN3roc,col="red")
lines(LDAroc,col="blue")
auc(KNN3roc)
auc(KNN9roc)
# ltrain=lda(y~x1+x2,data=train)
# lpred=predict(object = ltrain, newdata = test)$posterior[,1]
# lroc=roc(response=test$y,lpred)
# plot(lroc)
# auc(lroc)
auc(LOGroc)
auc(KNN3roc)
auc(LDAroc)
auc(LOGroc)
auc(KNN3roc)
auc(LDAroc)
