---
title: "compulsory_exercise_1_problem3.rmd"
output:
  pdf_document: default
  html_document: default
---

```{r setup, echo=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(ggplot2)
library(GGally)
library(class)
library(MASS)
library(pROC)

wine=read.csv("https://www.math.ntnu.no/emner/TMA4268/2018v/data/Comp1Wine.csv",sep=" ")
wine$class=as.factor(wine$class-1)
colnames(wine)=c("y","x1","x2")


n=dim(wine)[1]
set.seed(4268) #to get the same order if you rerun - but you change this to your favorite number
ord = sample(1:n) #shuffle 
test = wine[ord[1:(n/2)],]
train = wine[ord[((n/2)+1):n],]
glm.fits<-glm(y~x1+x2, family = binomial,data=train)
summary(glm.fits)
#coef(glm.fits)

```


## ## Oppgave 3, classification
#a)
* By standard algebraic rules for the logarithm one can show that logit$(p_i)$ is linear. 

$$
\text{logit}(p_i)=\log ( \frac{p_i}{1-p_i})=\log(p_i)-\log(1-p_i)=\log(\frac{e^{\beta_0 + \beta_1x_{i1} + \beta_2 x_{i2}}}{ 1+ e^{\beta_0 + \beta_1x_{i1} + \beta_2x_{i2}}})-\log(1-\frac{e^{\beta_0 + \beta_1x_{i1} + \beta_2 x_{i2}}}{ 1+ e^{\beta_0 + \beta_1x_{i1} + \beta_2x_{i2}}})= \log(\frac{e^{\beta_0 + \beta_1x_{i1} + \beta_2 x_{i2}}}{1+ e^{\beta_0 + \beta_1x_{i1} + \beta_2x_{i2}}} ) - \log(\frac{1+e^{\beta_0 + \beta_1x_{i1} + \beta_2 x_{i2}}-e^{\beta_0 + \beta_1x_{i1} + \beta_2x_{i2}}}{1+ e^{\beta_0 + \beta_1x_{i1} + \beta_2x_{i2}}})=\\=\log(e^{\beta_0 + \beta_1x_{i1} + \beta_2x_{i2}})-\log(1+e^{\beta_0 + \beta_1x_{i1} + \beta_2x_{i2}})-[\log(1)-\log(1+e^{\beta_0 + \beta_1x_{i1} + \beta_2x_{i2}})]=\log(e^{\beta_0 + \beta_1x_{i1} + \beta_2x_{i2}})=\beta_0 + \beta_1x_{i1} + \beta_2x_{i2}
$$
*A logistic regression model was fitted using the glm-function in R. In our case, the following values was found for the regression coefficients: 

-$\hat\beta_0\approx 0.4909$
-$\hat\beta_1\approx 0.4109$
-$\hat\beta\approx -1.8828$

* interpretation of $\hat\beta_1$ and $\hat\beta_2$: 
In the logistic regression setting, it can be more useful to consider the odds, $p_i/1-p_i$, rather than the probability $p_i$ as p_i is an exponential function. In our case, the odds is the conditional probability that a wine is in class 1 divided by the probability that it is in class 0, given its specific values for the two covariates. 

As shown above, logit($p_i$) is a linear function in the two covariates $x_{i1}$ and $x_{i2}$. If we keep one of the covariates constant, say $x_{i2}$ and increase $x_i1$ by 1 unit, logit($p_i$) will increase linearly with $\beta_1$. This means the odds itself will be multiplied with $exp(\beta_1)$. In other words, if for now only considering the alkalinity of ash in wine, a higher value for the covariate measuring this will give a higher odds for the wine being in class 1. Similarly, since $\beta_2$ is negative, an increase in the color intensity of a wine will give a decrease in the odds of the wine being in class 1, i.e. more likely to be in class 2. 

*[??] By using the rule $\text{Pr}(Y_i = 1|{\bf x}) = p_i > 0.5$ for classifying an observation to class 1, the following rule was found:

\begin{align}

0.5$> \frac{e^{\beta_0 + \beta_1x_{i1} + \beta_2 x_{i2}}}{ 1+ e^{\beta_0 + \beta_1x_{i1} + \beta_2x_{i2}}} \\
0.5\cdot ({ 1+ e^{\beta_0 + \beta_1x_{i1} + \beta_2x_{i2}}})>e^{\beta_0 + \beta_1x_{i1} + \beta_2 x_{i2}} \\
0.5$>0.5\cdot e^{\beta_0 + \beta_1x_{i1} + \beta_2 x_{i2}} \\ 
\log(1)$>\beta_0 + \beta_1x_{i1} + \beta_2 x_{i2} \\
x_{i2}$<-\beta_0-\beta_1x_1

\end{align}

* Figure \ref(??) shows the plot of the training data, in addition to the boundary line found above.
```{r, echo=FALSE}
b0=glm.fits$coefficients[1]
b1=glm.fits$coefficients[2]
b2=glm.fits$coefficients[3]
#plotting training data
#plot(train$y)
#a=(log(0.5)-b0)/b2
b=-b1/b2
a=(-b0)/b2

p<-ggplot(train,mapping=aes(x=x1,y=x2,color=y))+ggtitle('Training observations')+geom_point() + geom_abline(slope=b, intercept=a)

p
```
i) Linear is ok, sett inn utregning + plot.

i) Lineær ok, sett inn utregning + plot.
ii) 
iii) Interpret beta 1 og 2?
iv)Formel fra boka + spm om boundary skal være lineær.
v) Sjekk x2=... er riktig
vi)ok
vii) Hvordan presentere alle sannsynlighetene?
viii) 

$$
\text{Pr}(Y_i = 1| {\bf X}={\bf x}_i) = p_i = \frac{e^{\beta_0 + \beta_1x_{i1} + \beta_2 x_{i2}}}{ 1+ e^{\beta_0 + \beta_1x_{i1} + \beta_2x_{i2}}}
$$
#b) K-nearest neighbor classifier 

#c) LDA (& QDA)

* $\pi_k=Pr(Y=k)$ is the prior probability, which in this case gives the probability of being in class 1 or 2.$f_k({\bf x})$ is the probability density function, given a class. 

$$\text{Pr}(Y=k | {\bf X}={\bf x}) = \frac{\pi_k f_k({\bf x})}{\sum_{l=1}^K \pi_l f_l({\bf x})},$$

$$f_k({\bf x}) = \frac{1}{(2 \pi)^{p/2}|\boldsymbol{\Sigma}|^{1/2}}e^{-\frac{1}{2}({\bf x}-\boldsymbol{\mu_k})^T \boldsymbol{\Sigma}^{-1}({\bf x}-\boldsymbol{\mu_k})}.$$
