---
title: "compulsory_exercise_1_problem3.rmd"
output:
  html_document:
    df_print: paged
  output: null
  pdf_document: null
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(warnings=FALSE)
knitr::opts_chunk$set(message =FALSE)
```

```{r, echo=FALSE}
library(ggplot2)
library(GGally)
library(class)
library(MASS)
library(pROC)

wine=read.csv("https://www.math.ntnu.no/emner/TMA4268/2018v/data/Comp1Wine.csv",sep=" ")
wine$class=as.factor(wine$class-1)
colnames(wine)=c("y","x1","x2")
#ggpairs(wine, ggplot2::aes(color=y))


```
```{r, echo=FALSE}
n=dim(wine)[1]
set.seed(4268) #to get the same order if you rerun - but you change this to your favorite number
ord = sample(1:n) #shuffle 
test = wine[ord[1:(n/2)],]
train = wine[ord[((n/2)+1):n],]
glm.fits<-glm(y~x1+x2, family = binomial,data=train)
#summary(glm.fits)
#coef(glm.fits)
```
##Oppgave 3, classification
#a)
* By standard algebraic rules for the logarithm one can show that logit$(p_i)$ is linear.

$$
\text{logit}(p_i)=\log ( \frac{p_i}{1-p_i})=\log(p_i)-\log(1-p_i) \\
=
\log(\frac{e^{\beta_0 + \beta_1x_{i1} + \beta_2 x_{i2}}}{ 1+ e^{\beta_0 + \beta_1x_{i1} + \beta_2x_{i2}}})-\log(1-\frac{e^{\beta_0 + \beta_1x_{i1} + \beta_2 x_{i2}}}{ 1+ e^{\beta_0 + \beta_1x_{i1} + \beta_2x_{i2}}}) \\
=
\log(\frac{e^{\beta_0 + \beta_1x_{i1} + \beta_2 x_{i2}}}{1+ e^{\beta_0 + \beta_1x_{i1} + \beta_2x_{i2}}} ) - \log(\frac{1+e^{\beta_0 + \beta_1x_{i1} + \beta_2 x_{i2}}-e^{\beta_0 + \beta_1x_{i1} + \beta_2x_{i2}}}{1+ e^{\beta_0 + \beta_1x_{i1} + \beta_2x_{i2}}})\\
=
\log(e^{\beta_0 + \beta_1x_{i1} + \beta_2x_{i2}})-\log(1+e^{\beta_0 + \beta_1x_{i1} + \beta_2x_{i2}})-[\log(1)-\log(1+e^{\beta_0 + \beta_1x_{i1} + \beta_2x_{i2}})]\\
=
\log(e^{\beta_0 + \beta_1x_{i1} + \beta_2x_{i2}})=\beta_0 + \beta_1x_{i1} + \beta_2x_{i2}\\
$$

* A logistic regression model was fitted using the glm-function in R. In our case, the following values was found for the regression coefficients:

$\hat\beta_0\approx 0.4909$ and $\hat\beta_1\approx 0.4109$ and $\hat\beta\approx -1.8828$. 

* interpretation of $\hat\beta_1$ and $\hat\beta_2$: 
In the logistic regression setting, it can be more useful to consider the odds, $p_i/1-p_i$, rather than the probability $p_i$ as p_i is an exponential function. In our case, the odds is the conditional probability that a wine is in class 1 divided by the probability that it is in class 0, given its specific values for the two covariates. 

As shown above, logit($p_i$) is a linear function in the two covariates $x_{i1}$ and $x_{i2}$. If we keep one of the covariates constant, say $x_{i2}$ and increase $x_i1$ by 1 unit, logit($p_i$) will increase linearly with $\beta_1$. This means the odds itself will be multiplied with $exp(\beta_1)$. In other words, if for now only considering the alkalinity of ash in wine, a higher value for the covariate measuring this will give a higher odds for the wine being in class 1. Similarly, since $\beta_2$ is negative, an increase in the color intensity of a wine will give a decrease in the odds of the wine being in class 1, i.e. more likely to be in class 2. 

* [??] By using the rule $\text{Pr}(Y_i = 1|{\bf x}) = p_i > 0.5$ for classifying an observation to class 1, the following rule was found:

$$
0.5> \frac{e^{\beta_0 + \beta_1x_{i1} + \beta_2 x_{i2}}}{ 1+ e^{\beta_0 + \beta_1x_{i1} + \beta_2x_{i2}}} \\
0.5\cdot ({ 1+ e^{\beta_0 + \beta_1x_{i1} + \beta_2x_{i2}}})>e^{\beta_0 + \beta_1x_{i1} + \beta_2 x_{i2}} \\
0.5>0.5\cdot e^{\beta_0 + \beta_1x_{i1} + \beta_2 x_{i2}} \\ 
\log(1)>\beta_0 + \beta_1x_{i1} + \beta_2 x_{i2} \\
x_{i2}<-\beta_0-\beta_1x_1
$$

* Figure \ref(??) shows the plot of the training data, in addition to the boundary line found above.

```{r, echo=FALSE}
b0=glm.fits$coefficients[1]
b1=glm.fits$coefficients[2]
b2=glm.fits$coefficients[3]
#plotting training data
#plot(train$y)
#a=(log(0.5)-b0)/b2
b=-b1/b2
a=(-b0)/b2

p<-ggplot(train,mapping=aes(x=x1,y=x2,color=y))+ggtitle('Training observations')+geom_point() + geom_abline(slope=b, intercept=a)

p
```
* Here the confusion table must be made.

i) Linear is ok, sett inn utregning + plot.

i) Lineær ok, sett inn utregning + plot.
ii) 
iii) Interpret beta 1 og 2?
iv)Formel fra boka + spm om boundary skal være lineær.
v) Sjekk x2=... er riktig
vi)ok
vii) Hvordan presentere alle sannsynlighetene?
viii)$$\text{Pr}(Y_i = 1| {\bf X}={\bf x}_i) = p_i = \frac{e^{\beta_0 + \beta_1x_{i1} + \beta_2 x_{i2}}}{ 1+ e^{\beta_0 + \beta_1x_{i1} + \beta_2x_{i2}}}$$
#b) K-nearest neighbor classifier 

#c) LDA (& QDA)

* $\pi_k=Pr(Y=k)$ is the prior probability, which in this case gives the probability of being in class 1 or 2.$\mu_k$ is the mean for the two wine classes, and $f_k({\bf x})$ is the probability density function, given a class. $\Sigma$ is the covariate matrix.

* We can estimate the prior probability $\pi_k$ for the wines by taking the fraction of obervations from class $k$: $\hat{\pi}_k = \frac{n_k}{n}$, where $n$ is the total number of observations. $\mu_k$ can be estimated from the sample mean:  
$$\hat{\boldsymbol{\mu}}_k = \frac{1}{n_k}\sum_{i:y_i=k} {\bf X}_i$$.

The $\Sigma$-matrix can be estimated for the two classes by summing up $$\hat{\boldsymbol{\Sigma}}_k=\frac{1}{n_k-1}\sum_{i:y_i=k} ({\bf X}_i-\hat{\boldsymbol{\mu}}_k ) ({\bf X}_i-\hat{\boldsymbol{\mu}}_k)^T$$, with $$\hat{\boldsymbol{\Sigma}}=\frac{(n_0 - 1) \cdot \hat{\boldsymbol{\Sigma}}_0 + (n_1 - 1) \cdot \hat{\boldsymbol{\Sigma}}_1}{n_1 + n_2 - 2}$$. Computing the estimates for these quantities from the train set, we get:

```{r, echo=FALSE}
library(dplyr)


n0=sum(train[,1]==1)
n1=nrow(train)-n0
pi=c(n0/n,n1/n)
x_01=train[train$y==0,"x1"]
x_02=train[train$y==0,"x2"]
x_11=train[train$y==1,"x1"]
x_12=train[train$y==1,"x2"]
mu_0=c(mean(x_01),mean(x_02))
mu_1=c(mean(x_11),mean(x_12))
Sigma_0=var(train[train$y==0,2:3])
Sigma_1=var(train[train$y==1,2:3])
Sigma_0
Sigma_1
Sigma=(n0-1)*Sigma_0/(n0+n1-2)+(n1-1)*Sigma_1/(n0+n1-2)
Sigma

```

```{r,,echo=FALSE,eval=TRUE}
print("Sigma:")
Sigma
print("mu_0")
mu_0
print("mu_1")
mu_1
print("pi")
pi

```
* The desicion boundary is where $P_0(\bf{x})=P_1(\bf{x})$, which can be rewritten to discriminant scores $\delta_0(\bf{x})$=$\delta_1(\bf{x})$ for the two wine classes.

$$
\text{Pr}(Y=0|{\bf X=x})=\text{Pr}(Y=1|{\bf X=x})\\
$$

$$
\pi_0 \frac{1}{(2 \pi)^{p/2}|\boldsymbol{\Sigma}|^{1/2} \sum_{l=1}^2 \pi_l f_l({\bf x}) }e^{-\frac{1}{2}({\bf x}-\boldsymbol{\mu_0})^T \boldsymbol{\Sigma}^{-1}({\bf x}-\boldsymbol{\mu_0})}
=
\pi_1 \frac{1}{(2 \pi)^{p/2}|\boldsymbol{\Sigma}|^{1/2} \sum_{l=1}^2 \pi_l f_l({\bf x})}e^{-\frac{1}{2}({\bf x}-\boldsymbol{\mu_1})^T \boldsymbol{\Sigma}^{-1}({\bf x}-\boldsymbol{\mu_1})} \\
$$
Taking the logarithms on each side gives:
$$
\log \pi_0 -\frac{1}{2}({\bf x}-\boldsymbol{\mu_0})^T \boldsymbol{\Sigma}^{-1}({\bf x}-\boldsymbol{\mu_0})
=
\log \pi_1-\frac{1}{2}({\bf x}-\boldsymbol{\mu_1})^T \boldsymbol{\Sigma}^{-1}({\bf x}-\boldsymbol{\mu_1})
$$

Since $\Sigma$ is symmetric positive definite, the two off-diagonal elements in $\Sigma$, $\sigma_12$ and $\sigma_21$ are equal. Then ${\bf x}^T\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu_k} = \boldsymbol{\mu_k}^T\boldsymbol{\Sigma}^{-1}{\bf x}$. Due to this and subtracting ${\bf x}^T\boldsymbol{\Sigma}^{-1}{\bf x}$ from both sides, the equation reduces to:

$$
\log \pi_0 -\frac{1}{2}\boldsymbol{\mu_0}^T \boldsymbol{\Sigma}^{-1}\boldsymbol{\mu_0} + {\bf x}^T{\Sigma}^{-1}\boldsymbol{\mu_0}
=
\log \pi_1 -\frac{1}{2}\boldsymbol{\mu_1}^T \boldsymbol{\Sigma}^{-1}\boldsymbol{\mu_1} + {\bf x}^T{\Sigma}^{-1}\boldsymbol{\mu_1}\\
$$

, which is $\delta_0(\bf{x})=\delta_1(\bf{x})$.

* The rule for classification can use the discriminant values. If $\delta_1(\bf{x})$ is greater than $\delta_0(\bf{x}$, i.e.

$$
\delta_1(\bf{x})-\delta_0(\bf{x})>0 \\
{\bf x}^T \boldsymbol{\Sigma}^{-1}\boldsymbol{(\mu_1-\mu_0)} - \frac{1}{2}\boldsymbol\mu_1^T \boldsymbol{\Sigma}^{-1}\boldsymbol\mu_1 + \frac{1}{2}\boldsymbol\mu_0^T \boldsymbol{\Sigma}^{-1}\boldsymbol\mu_0 + \log \frac{\pi_1}{\pi_0} > 0\\

$$

then $\boldsymbol{x}$ is assigned to class 1.

$$
{\bf x}^T \boldsymbol{\Sigma}^{-1}\boldsymbol{(\mu_1-\mu_0)} =\frac{1}{2}\boldsymbol\mu_1^T \boldsymbol{\Sigma}^{-1}\boldsymbol\mu_1 - \frac{1}{2}\boldsymbol\mu_0^T \boldsymbol{\Sigma}^{-1}\boldsymbol\mu_0 - \log \frac{\pi_1}{\pi_0} \\
$$
Solving this equation with the estimates for $\boldsymbol{\mu}$ and $\boldsymbol{\Sigma}$, gives the $\bf{x}$-values needed to find the boundary line. 


```{r,echo=FALSE}
mu_1
t(mu_1)
Sigma
right_side=0.5*t(mu_1)%*%solve(Sigma)%*%mu_1-0.5*t(mu_0)%*%solve(Sigma)%*%mu_0-log(pi[2]/pi[1])
left_side=solve(Sigma)%*%(mu_1-mu_0)
const_1=left_side[1]
const_2=left_side[2]
train_intercept=right_side/const_2
train_slope=-const_1/const_2
print(train_intercept)
print(train_slope)
```

This gives the following equation for the boundary line: $0.164x_1-2.628x_2=-8.368 \rightarrow x_2=3.184+0.063x_1$.

```{r,echo=FALSE}
library(MASS)

library(ggplot2)

#Plotting train and test-data. 
train_plot=ggplot(train,aes(x=x1,y=x2,color=y))+geom_point(size=2.5)+geom_abline(slope=train_slope,intercept=train_intercept,color="green3")+geom_point(data=test,shape=5,size=2.5)
train_plot

#Performed LDA
lda.train=lda(y~x1+x2,data=train)
lda.train_pred=predict(lda.train,newdata=test)
lda.train_pred
conf_table=table(lda.train_pred$class,test$y)

```

```{r,echo=FALSE}
#Creating confusion table
library(knitr)
library(kableExtra)

row1 = c("", "Predicted -", "Predicted +","Total")
row2 = c("True -", conf_table[1,1], conf_table[1,2],sum(conf_table[1,]))
row3 = c("True +", conf_table[2,1], conf_table[2,2],sum(conf_table[2,]))
row4 = c("Total", sum(conf_table[,1]), sum(conf_table[,2])," ")
table_wine=kable(rbind(row2, row3,row4),col.names=row1,row.names=FALSE)
table_wine

total=sum(conf_table)
ratio=(conf_table[1,1]+conf_table[2,2])/total
test_error=(conf_table[1,2]+conf_table[2,1])/total

#Calculating sensitivity and specificity
wine_sens=conf_table[2,2]/sum(conf_table[2,])
wine_spec=conf_table[1,1]/sum(conf_table[1,])
print("Sensitivity: ")
print(wine_sens)
print("Specificity: ")
print(wine_spec)
print(ratio)
print(test_error)
```
* The sensitivity is 0.833, and the specificity is 0.828. This is quite good, the fraction of correct predictions is 0.831, wih a testerror of 0.17.
```


```{r, eval=FALSE}
library(pROC)
glmroc=roc(response=test$y,predictor=predglm)
plot(glmroc)
auc(glmroc)

KNN3 = knn(train = train[,-1], test = test[,-1], k = 3, cl = train$y, prob = F)
KNN3probwinning = attributes(knn(train = train[,-1], test = test[,-1], k = 3, cl = train$y, prob = TRUE))$prob
KNN3prob <- ifelse(KNN3 == "0", 1-KNN3probwinning, KNN3probwinning)
KNN3roc=roc(response=test$y,predictor=KNN3prob) 

plot(KNN3roc)
auc(KNN3roc)
ltrain=lda(y~x1+x2,data=train)
lpred=predict(object = ltrain, newdata = test)$posterior[,1]
lroc=roc(response=test$y,lpred)
plot(lroc)
auc(lroc)
```
```


